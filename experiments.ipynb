{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "GWvNv3IrQtwK"
   },
   "outputs": [],
   "source": [
    "# Torch-related imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "\n",
    "# Scikit-learn-related imports\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# Nibabel and Scipy imports (for handling fMRI and image processing)\n",
    "import nibabel as nib\n",
    "import scipy.ndimage as ndimage  # For smoothing\n",
    "\n",
    "# NumPy, Matplotlib, and Seaborn (for data manipulation and visualization)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# OS for file system operations\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 219,
     "status": "ok",
     "timestamp": 1729905899637,
     "user": {
      "displayName": "Yasty Sánchez",
      "userId": "01336246420740504937"
     },
     "user_tz": 360
    },
    "id": "PBnx2TrBRAvy",
    "outputId": "7abe6154-a5d7-4cdc-b770-78d6db615511"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '0'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3693,
     "status": "ok",
     "timestamp": 1729906065409,
     "user": {
      "displayName": "Yasty Sánchez",
      "userId": "01336246420740504937"
     },
     "user_tz": 360
    },
    "id": "ELtPOdZYQ9ny",
    "outputId": "2975f1d9-50c4-42d0-ca16-e5c337366126"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total file paths: 5100\n",
      "Total labels: 5100\n",
      "Class weights:  tensor([0.8004, 1.3323], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# root_dir = os.path.join('/content/drive', 'My Drive', 'UCR', '2-2024', 'InvCC', 'ADHD200', 'Datasets', 'preprocessed')\n",
    "\n",
    "import os\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "root_dir = os.path.join('data', 'preprocessed')\n",
    "\n",
    "tdc_dir = os.path.join(root_dir, 'TDC')\n",
    "adhd_dir = os.path.join(root_dir, 'ADHD')\n",
    "\n",
    "# To save autoencoder state dict\n",
    "save_path = os.path.join(root_dir, 'autoencoder.pt')\n",
    "\n",
    "# Recursively find all .nii.gz files in TDC and ADHD folders\n",
    "tdc_file_paths = [\n",
    "    os.path.join(root, file)\n",
    "    for root, _, files in os.walk(tdc_dir)\n",
    "    for file in files if file.endswith('.nii.gz')\n",
    "]\n",
    "\n",
    "adhd_file_paths = [\n",
    "    os.path.join(root, file)\n",
    "    for root, _, files in os.walk(adhd_dir)\n",
    "    for file in files if file.endswith('.nii.gz')\n",
    "]\n",
    "\n",
    "# Assuming tdc_file_paths and adhd_file_paths were correctly populated\n",
    "tdc_labels = [0] * len(tdc_file_paths)  # Create labels for TDC\n",
    "adhd_labels = [1] * len(adhd_file_paths)  # Create labels for ADHD\n",
    "\n",
    "# Combine file paths and labels\n",
    "file_paths = tdc_file_paths + adhd_file_paths\n",
    "labels = tdc_labels + adhd_labels\n",
    "\n",
    "# Verify lengths\n",
    "print(f\"Total file paths: {len(file_paths)}\")  # Should be 5100\n",
    "print(f\"Total labels: {len(labels)}\")  # Should also be 5100\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(labels), y=labels)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "print(f'Class weights: ', class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rzMlmaGzUNdd"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "_6Oj7oNrUHLO"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import nibabel as nib\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "\n",
    "class FMRI_Dataset(Dataset):\n",
    "    def __init__(self, file_paths, labels, max_shape, smoothing_sigma=1, augment=False):\n",
    "        self.file_paths = file_paths  # List of paths to the fMRI data files\n",
    "        self.labels = labels  # Corresponding labels\n",
    "        self.max_shape = max_shape  # Shape to pad all inputs to (e.g., [1, 61, 73, 61])\n",
    "        self.smoothing_sigma = smoothing_sigma  # Standard deviation for Gaussian smoothing\n",
    "        self.augment = augment  # Apply augmentations if True\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load fMRI data\n",
    "        fmri_img = nib.load(self.file_paths[idx])\n",
    "        data = fmri_img.get_fdata()\n",
    "\n",
    "        # Apply Gaussian smoothing\n",
    "        data = self.smooth_data(data)\n",
    "\n",
    "        # Apply augmentations if enabled\n",
    "        if self.augment:\n",
    "            data = self.apply_augmentations(data)\n",
    "\n",
    "        # Normalize the data\n",
    "        data = self.normalize_data(data)\n",
    "\n",
    "        # Convert to tensor and add missing dimensions\n",
    "        data_tensor = torch.tensor(data, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        # Pad the tensor to the specified max_shape\n",
    "        data_padded = F.pad(data_tensor, pad=self.calculate_padding(data_tensor.shape), mode='constant', value=0)\n",
    "\n",
    "        # Ensure the final shape matches max_shape\n",
    "        data_padded = data_padded.view(*self.max_shape)\n",
    "\n",
    "        # Get the label\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "        return data_padded, label\n",
    "\n",
    "    def apply_augmentations(self, data):\n",
    "        data = self.add_noise(data)\n",
    "        data = self.random_rotate(data)\n",
    "        data = self.random_intensity_shift(data)\n",
    "        return data\n",
    "\n",
    "    def add_noise(self, data, mean=0, std=0.01):\n",
    "        noise = np.random.normal(mean, std, data.shape)\n",
    "        return data + noise\n",
    "\n",
    "    def random_rotate(self, data):\n",
    "        angles = np.random.uniform(-5, 5, size=3)\n",
    "        return ndimage.rotate(data, angle=angles[0], axes=(1, 2), reshape=False, mode='nearest')\n",
    "\n",
    "    def random_intensity_shift(self, data, shift_limit=0.05):\n",
    "        shift_value = np.random.uniform(-shift_limit, shift_limit)\n",
    "        return data + shift_value\n",
    "\n",
    "    def calculate_padding(self, current_shape):\n",
    "        padding = []\n",
    "        for current_dim, max_dim in zip(reversed(current_shape), reversed(self.max_shape)):\n",
    "            pad_total = max_dim - current_dim\n",
    "            padding.append(pad_total // 2)\n",
    "            padding.append(pad_total - (pad_total // 2))\n",
    "        return padding\n",
    "\n",
    "    def normalize_data(self, data):\n",
    "        mean = data.mean()\n",
    "        std = data.std()\n",
    "        return (data - mean) / std if std > 0 else data\n",
    "\n",
    "    def smooth_data(self, data):\n",
    "        return ndimage.gaussian_filter(data, sigma=self.smoothing_sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "n8wMvOSlXa0N",
    "outputId": "17ebcb4d-69c9-4232-8cfc-980c8e921933"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 3570\n",
      "Validation set size: 765\n",
      "Test set size: 765\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Assuming you already have file_paths and labels defined as in your previous code\n",
    "\n",
    "# Parameters\n",
    "batch_size = 4\n",
    "num_classes = 2\n",
    "max_shape = [1, 61, 73, 61]\n",
    "\n",
    "# Stratified Shuffle Split\n",
    "labels = np.array(labels)\n",
    "dataset = FMRI_Dataset(file_paths, labels, max_shape, augment=True)\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)  # 70% train, 30% test\n",
    "\n",
    "for train_index, test_index in sss.split(file_paths, labels):\n",
    "    train_file_paths, test_file_paths = np.array(file_paths)[train_index], np.array(file_paths)[test_index]\n",
    "    train_labels, test_labels = labels[train_index], labels[test_index]\n",
    "\n",
    "# Further split the test set into validation and test sets\n",
    "sss_val = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)  # 50% of the test set for validation\n",
    "for val_index, test_index in sss_val.split(test_file_paths, test_labels):\n",
    "    val_file_paths, final_test_file_paths = np.array(test_file_paths)[val_index], np.array(test_file_paths)[test_index]\n",
    "    val_labels, final_test_labels = test_labels[val_index], test_labels[test_index]\n",
    "\n",
    "# Print the results\n",
    "print(f\"Training set size: {len(train_file_paths)}\")\n",
    "print(f\"Validation set size: {len(val_file_paths)}\")\n",
    "print(f\"Test set size: {len(final_test_file_paths)}\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = FMRI_Dataset(train_file_paths.tolist(), train_labels.tolist(), max_shape)\n",
    "val_dataset = FMRI_Dataset(val_file_paths.tolist(), val_labels.tolist(), max_shape)\n",
    "test_dataset = FMRI_Dataset(final_test_file_paths.tolist(), final_test_labels.tolist(), max_shape)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Path: data/preprocessed/TDC/ADHD200_DPARSF_ADHD200_Pittsburgh_0016088_HC/zVMHCMap_ADHD200_Pittsburgh_0016088.nii.gz\n",
      "Label: 0\n",
      "Data Shape: (61, 73, 61)\n",
      "\n",
      "File Path: data/preprocessed/ADHD/ADHD200_DPARSF_ADHD200_Peking_1_3390312_ADHD/DegreeCentrality_PositiveBinarizedSumBrainMap_ADHD200_Peking_1_3390312.nii.gz\n",
      "Label: 1\n",
      "Data Shape: (61, 73, 61)\n",
      "\n",
      "File Path: data/preprocessed/ADHD/ADHD200_DPARSF_ADHD200_Peking_1_8463326_ADHD/DegreeCentrality_PositiveWeightedSumBrainMap_ADHD200_Peking_1_8463326.nii.gz\n",
      "Label: 1\n",
      "Data Shape: (61, 73, 61)\n",
      "\n",
      "File Path: data/preprocessed/ADHD/ADHD200_DPARSF_ADHD200_NeuroIMAGE_0027023_ADHD/DegreeCentrality_PositiveWeightedSumBrainMap_ADHD200_NeuroIMAGE_0027023.nii.gz\n",
      "Label: 1\n",
      "Data Shape: (61, 73, 61)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Sample based on actual length\n",
    "sample_size = min(4, len(file_paths))  # Adjust sample size to available data\n",
    "sample_indices = random.sample(range(len(file_paths)), sample_size)\n",
    "\n",
    "for idx in sample_indices:\n",
    "    print(f\"File Path: {file_paths[idx]}\")\n",
    "    print(f\"Label: {labels[idx]}\")\n",
    "    data = nib.load(file_paths[idx]).get_fdata()\n",
    "    print(f\"Data Shape: {data.shape}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN-AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 61, 73, 61])\n"
     ]
    }
   ],
   "source": [
    "class CNN_Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_Autoencoder, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv3d(1, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(256, 512, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose3d(512, 256, kernel_size=3, stride=2, padding=1, output_padding=(1, 1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose3d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=(1, 0, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose3d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=(0, 0, 0)),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose3d(64, 1, kernel_size=3, stride=2, padding=1, output_padding=(0, 0, 0)),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "autoencoder = CNN_Autoencoder().to(device)\n",
    "\n",
    "# Generate random input matching new shape [1, 1, 53, 64] (Batch size 1)\n",
    "inputs = torch.rand((1, 61, 73, 61)).to(device)  # Example input\n",
    "output = autoencoder(inputs)\n",
    "print(output.shape)  # should match the input shape [1, 53, 64, 46]\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()  # Since it's an autoencoder, Mean Squared Error is commonly used\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=0.001, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EYw0MT13SblI"
   },
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S-BF1plyaGTM"
   },
   "source": [
    "### Using pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "HR232TTAWkLH"
   },
   "outputs": [],
   "source": [
    "# Model definition\n",
    "\n",
    "class CNNOnEncoder(nn.Module):\n",
    "    def __init__(self, autoencoder, num_classes):\n",
    "        super(CNNOnEncoder, self).__init__()\n",
    "        self.encoder = autoencoder.encoder  # Use the encoder from the autoencoder\n",
    "        self.conv1 = nn.Conv3d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv3d(in_channels=512, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv3d(in_channels=256, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.AdaptiveMaxPool3d(output_size=(8, 8, 5))\n",
    "        self.fc1 = nn.Linear(128 * 8 * 8 * 5, 256)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        x = self.encoder(x)\n",
    "        # print(f'Encoder {x.shape}')\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # First conv + pooling\n",
    "        # print(f'First conv + pooling {x.shape}')\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # Second conv + pooling\n",
    "        # print(f'Second conv + pooling {x.shape}')\n",
    "        x = self.pool(F.relu(self.conv3(x)))  # Second conv + pooling\n",
    "        x = x.unsqueeze(0)\n",
    "        x = x.view(x.size(1), -1)  # Flatten\n",
    "        # print(f'Flattened {x.shape}')\n",
    "        x = F.relu(self.fc1(x))  # First fully connected layer\n",
    "        x = self.fc2(x)  # Output layer\n",
    "        x = F.softmax(x, dim=1)  # Apply softmax for probabilistic output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rl5qWwF7aAIi"
   },
   "outputs": [],
   "source": [
    "# Load trained autoencoder\n",
    "trained_autoencoder = CNN_Autoencoder()\n",
    "trained_autoencoder.load_state_dict(torch.load(save_path, weights_only=True))\n",
    "trained_autoencoder.to(device)\n",
    "trained_autoencoder.eval() # Set the model to evaluation mode\n",
    "\n",
    "# Create an instance of the new model\n",
    "cnn_with_ae_model = CNNOnEncoder(trained_autoencoder, num_classes=2).to(device)\n",
    "\n",
    "# Optionally, freeze the encoder layers, Frozen= false, Unfrozen= true\n",
    "for param in cnn_with_ae_model.encoder.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = torch.optim.SGD(cnn_with_ae_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 216
    },
    "executionInfo": {
     "elapsed": 255,
     "status": "error",
     "timestamp": 1725738817442,
     "user": {
      "displayName": "Yasty Sánchez",
      "userId": "01336246420740504937"
     },
     "user_tz": 360
    },
    "id": "JtGhl4d5WuQ8",
    "outputId": "8ce13504-e388-4ebe-f475-51f24117dc37"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[116], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mcnn_with_ae_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, labels)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[114], line 16\u001b[0m, in \u001b[0;36mCNNOnEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# print(x.shape)\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# print(f'Encoder {x.shape}')\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)))  \u001b[38;5;66;03m# First conv + pooling\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:608\u001b[0m, in \u001b[0;36mConv3d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:603\u001b[0m, in \u001b[0;36mConv3d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    592\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv3d(\n\u001b[1;32m    593\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    594\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    602\u001b[0m     )\n\u001b[0;32m--> 603\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv3d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    cnn_with_ae_model.train()\n",
    "    total_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        # Send inputs and labels to the device (GPU if available)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output = cnn_with_ae_model(inputs)\n",
    "\n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate the loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Store predictions and labels for accuracy calculation\n",
    "        _, preds = torch.max(output, 1)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    # Calculate training accuracy and average loss for the epoch\n",
    "    train_accuracy = accuracy_score(all_labels, all_preds)\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}')\n",
    "\n",
    "    # Validation phase\n",
    "    cnn_with_ae_model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_labels = []\n",
    "    val_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_labels_batch in val_loader:\n",
    "            val_inputs = val_inputs.to(device)\n",
    "            val_labels_batch = val_labels_batch.to(device)\n",
    "\n",
    "            # Forward pass for validation data\n",
    "            val_outputs = cnn_with_ae_model(val_inputs)\n",
    "            loss = criterion(val_outputs, val_labels_batch)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Store predictions and labels for accuracy calculation\n",
    "            _, val_preds_batch = torch.max(val_outputs, 1)\n",
    "            val_labels.extend(val_labels_batch.cpu().numpy())\n",
    "            val_preds.extend(val_preds_batch.cpu().numpy())\n",
    "\n",
    "    # Calculate validation accuracy and average loss\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = accuracy_score(val_labels, val_preds)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "    # Save the model if validation loss improves\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(cnn_with_ae_model.state_dict(), 'best_cnn_with_ae.pt')\n",
    "        print(f'Model saved at epoch {epoch+1} with validation loss: {avg_val_loss:.4f}')\n",
    "\n",
    "    # Save the model after each epoch\n",
    "    torch.save(cnn_with_ae_model.state_dict(), f'cnn_with_ae_epoch{epoch+1}.pt')\n",
    "\n",
    "# Save the final trained model\n",
    "torch.save(cnn_with_ae_model.state_dict(), 'cnn_with_ae_final.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NC_sALTGWwog"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the CNN with autoencoder on the test images: 52.23%\n",
      "Average loss on the test images: 0.6923\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Testing loop\n",
    "\n",
    "# Load trained autoencoder\n",
    "trained_autoencoder = CNN_Autoencoder()\n",
    "trained_autoencoder.load_state_dict(torch.load(save_path, weights_only=True))\n",
    "trained_autoencoder.to(device)\n",
    "trained_autoencoder.eval() # Set the model to evaluation mode\n",
    "\n",
    "# Load the model\n",
    "num_classes = 2\n",
    "\n",
    "classifier_model = CNNOnEncoder(trained_autoencoder, num_classes=2).to(device)  # Initialize your classifier model\n",
    "model_state_dict = torch.load('cnn_with_ae_final.pt', weights_only=False)\n",
    "classifier_model.load_state_dict(model_state_dict)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "classifier_model.eval()\n",
    "\n",
    "# Define criterion (loss function)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Test the model\n",
    "correct = 0\n",
    "total = 0\n",
    "total_loss = 0.0\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        # Send inputs and labels to the device (GPU if available)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output = classifier_model(inputs)\n",
    "        # print(outputs.shape)\n",
    "\n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Store predictions and labels for confusion matrix\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Calculate accuracy and average loss\n",
    "accuracy = 100 * correct / total\n",
    "average_loss = total_loss / len(test_loader)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# Plot confusion matrix using seaborn\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax,\n",
    "            xticklabels=[f'Class {i}' for i in range(num_classes)],\n",
    "            yticklabels=[f'Class {i}' for i in range(num_classes)])\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('True')\n",
    "ax.set_title('Confusion Matrix')\n",
    "\n",
    "# Print accuracy and average loss\n",
    "print(f'Accuracy of the CNN with autoencoder on the test images: {accuracy:.2f}%')\n",
    "print(f'Average loss on the test images: {average_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EhOKd9tEVyhT"
   },
   "source": [
    "### Not using pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BrAqWJi3ZhnE"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm3d(64)  # Added BatchNorm layer\n",
    "        self.conv2 = nn.Conv3d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm3d(128)  # Added BatchNorm layer\n",
    "        self.conv3 = nn.Conv3d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm3d(256)  # Added BatchNorm layer\n",
    "        self.pool = nn.AdaptiveMaxPool3d(output_size=(8, 8, 5))\n",
    "        self.fc1 = nn.Linear(256 * 8 * 8 * 5, 256)\n",
    "        self.dropout = nn.Dropout(p=0.5)  # Added dropout\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "        # Initialize weights\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, (nn.Conv3d, nn.Linear)):\n",
    "            nn.init.kaiming_uniform_(m.weight, a=nn.init.calculate_gain('relu'))  # Use He initialization\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)  # Initialize biases to zero\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)  # Added dropout layer\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "tW3TORHwZmS7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([4, 2])\n"
     ]
    }
   ],
   "source": [
    "simple_cnn_model = SimpleCNN(num_classes=2).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = torch.optim.Adam(simple_cnn_model.parameters(), lr=0.001)\n",
    "\n",
    "example_input = torch.randn(4, 1, 61, 73, 61)\n",
    "example_output = simple_cnn_model(example_input.to(device))\n",
    "print(\"Output shape:\", example_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "Hbl8nB7YV2yZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Train Loss: 9.6746, Train Accuracy: 0.6160\n",
      "Epoch [1/100], Val Loss: 0.6757, Val Accuracy: 0.6614\n",
      "Model saved at epoch 1 with validation loss: 0.6757\n",
      "Epoch [2/100], Train Loss: 0.6838, Train Accuracy: 0.6496\n",
      "Epoch [2/100], Val Loss: 0.6632, Val Accuracy: 0.6614\n",
      "Model saved at epoch 2 with validation loss: 0.6632\n",
      "Epoch [3/100], Train Loss: 0.7174, Train Accuracy: 0.6280\n",
      "Epoch [3/100], Val Loss: 4.6413, Val Accuracy: 0.6248\n",
      "Epoch [4/100], Train Loss: 0.7341, Train Accuracy: 0.6241\n",
      "Epoch [4/100], Val Loss: 0.6912, Val Accuracy: 0.6248\n",
      "Epoch [5/100], Train Loss: 0.6911, Train Accuracy: 0.6246\n",
      "Epoch [5/100], Val Loss: 0.6912, Val Accuracy: 0.6248\n",
      "Epoch [6/100], Train Loss: 0.6906, Train Accuracy: 0.6246\n",
      "Epoch [6/100], Val Loss: 0.6912, Val Accuracy: 0.6248\n",
      "Epoch [7/100], Train Loss: 0.6912, Train Accuracy: 0.6246\n",
      "Epoch [7/100], Val Loss: 0.6912, Val Accuracy: 0.6248\n",
      "Epoch [8/100], Train Loss: 0.6911, Train Accuracy: 0.6246\n",
      "Epoch [8/100], Val Loss: 0.6912, Val Accuracy: 0.6248\n",
      "Epoch [9/100], Train Loss: 0.7302, Train Accuracy: 0.6244\n",
      "Epoch [9/100], Val Loss: 0.6912, Val Accuracy: 0.6248\n",
      "Epoch [10/100], Train Loss: 0.8125, Train Accuracy: 0.6235\n",
      "Epoch [10/100], Val Loss: 0.6912, Val Accuracy: 0.6248\n",
      "Epoch [11/100], Train Loss: 0.6914, Train Accuracy: 0.6246\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[113], line 59\u001b[0m\n\u001b[1;32m     57\u001b[0m val_outputs \u001b[38;5;241m=\u001b[39m simple_cnn_model(val_inputs)\n\u001b[1;32m     58\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(val_outputs, val_labels_batch)\n\u001b[0;32m---> 59\u001b[0m val_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Store predictions and labels for accuracy calculation\u001b[39;00m\n\u001b[1;32m     62\u001b[0m _, val_preds_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(val_outputs, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "\n",
    "num_epochs = 100\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    simple_cnn_model.train()\n",
    "    total_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        # Send inputs and labels to the device (GPU if available)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output = simple_cnn_model(inputs)\n",
    "\n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate the loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Store predictions and labels for accuracy calculation\n",
    "        _, preds = torch.max(output, 1)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    # Calculate training accuracy and average loss for the epoch\n",
    "    train_accuracy = accuracy_score(all_labels, all_preds)\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}')\n",
    "\n",
    "    # Validation phase\n",
    "    simple_cnn_model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_labels = []\n",
    "    val_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_labels_batch in val_loader:\n",
    "            val_inputs = val_inputs.to(device)\n",
    "            val_labels_batch = val_labels_batch.to(device)\n",
    "\n",
    "            # Forward pass for validation data\n",
    "            val_outputs = simple_cnn_model(val_inputs)\n",
    "            loss = criterion(val_outputs, val_labels_batch)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Store predictions and labels for accuracy calculation\n",
    "            _, val_preds_batch = torch.max(val_outputs, 1)\n",
    "            val_labels.extend(val_labels_batch.cpu().numpy())\n",
    "            val_preds.extend(val_preds_batch.cpu().numpy())\n",
    "\n",
    "    # Calculate validation accuracy and average loss\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = accuracy_score(val_labels, val_preds)\n",
    "\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "    # Save the model if validation loss improves\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(simple_cnn_model.state_dict(), 'best_simple_cnn.pt')\n",
    "        print(f'Model saved at epoch {epoch+1} with validation loss: {avg_val_loss:.4f}')\n",
    "\n",
    "    # Save the model after each epoch\n",
    "    torch.save(simple_cnn_model.state_dict(), f'simple_cnn_epoch{epoch+1}.pt')\n",
    "\n",
    "# Save the final trained model\n",
    "torch.save(simple_cnn_model.state_dict(), 'simple_cnn_final.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6tjD7zs4ZtyY"
   },
   "outputs": [],
   "source": [
    "# Testing loop\n",
    "\n",
    "# Load the model\n",
    "classifier_model = SimpleCNN(num_classes=2).to(device)  # Initialize your classifier model\n",
    "#model_state_dict = torch.load('simple_cnn_final.pt', weights_only=False)\n",
    "#classifier_model.load_state_dict(model_state_dict)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "classifier_model.eval()\n",
    "\n",
    "# Define criterion (loss function)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Test the model\n",
    "correct = 0\n",
    "total = 0\n",
    "total_loss = 0.0\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        # Send inputs and labels to the device (GPU if available)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output = simple_cnn_model(inputs)\n",
    "        # print(outputs.shape)\n",
    "\n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Store predictions and labels for confusion matrix\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Calculate accuracy and average loss\n",
    "accuracy = 100 * correct / total\n",
    "average_loss = total_loss / len(test_loader)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# Plot confusion matrix using seaborn\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax,\n",
    "            xticklabels=[f'Class {i}' for i in range(num_classes)],\n",
    "            yticklabels=[f'Class {i}' for i in range(num_classes)])\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('True')\n",
    "ax.set_title('Confusion Matrix')\n",
    "\n",
    "# Print accuracy and average loss\n",
    "print(f'Accuracy of the simple CNN on the test images: {accuracy:.2f}%')\n",
    "print(f'Average loss on the test images: {average_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J2w7Mw4rSe2U"
   },
   "source": [
    "# CNN-LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hv-DovH-a-AR"
   },
   "source": [
    "### Using pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WKoBTxICbFxY"
   },
   "outputs": [],
   "source": [
    "class CNNLSTMOnAutoencoder(nn.Module):\n",
    "    def __init__(self, autoencoder, hidden_size, num_classes):\n",
    "        super(CNNLSTMOnAutoencoder, self).__init__()\n",
    "        self.encoder = autoencoder.encoder\n",
    "        self.conv1 = nn.Conv3d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv3d(in_channels=512, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv3d(in_channels=256, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.AdaptiveMaxPool3d(output_size=(8, 8, 5))\n",
    "        self.lstm = None\n",
    "        self.fc = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        x = self.encoder(x)\n",
    "        # Apply convolutions, dropout, and pooling layers\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        # print(f'First conv + pooling {x.shape}')\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        # print(f'Second conv + pooling {x.shape}')\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        # print(f'Third conv + pooling {x.shape}')\n",
    "\n",
    "        # Get the final shape after convolutions\n",
    "        conv_output_shape = x.shape\n",
    "        # print(f\"Convolution output shape: {conv_output_shape}\")\n",
    "        \n",
    "        # Extract available dimensions for reshaping\n",
    "        batch_size, channels, new_height, new_width, new_depth = conv_output_shape\n",
    "\n",
    "        # Calculate the LSTM input size\n",
    "        lstm_input_size = channels * new_height * new_width\n",
    "\n",
    "        # Reshape for LSTM input\n",
    "        x = x.view(batch_size, new_depth, lstm_input_size)\n",
    "\n",
    "        # Initialize LSTM and FC layers if not yet initialized\n",
    "        if self.lstm is None:\n",
    "            self.lstm = nn.LSTM(input_size=lstm_input_size, hidden_size=128, batch_first=True).to(x.device)\n",
    "            self.fc = nn.Linear(in_features=128, out_features=2).to(x.device)\n",
    "\n",
    "        # Pass through LSTM and FC\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        out = F.relu(self.fc(lstm_out[:, -1, :]))\n",
    "        out = F.softmax(out, dim=1)  # Apply softmax for probabilistic output\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained autoencoder\n",
    "trained_autoencoder = CNN_Autoencoder().to(device)\n",
    "trained_autoencoder.load_state_dict(torch.load(save_path, weights_only=True))\n",
    "trained_autoencoder.eval() # Set the model to evaluation mode\n",
    "\n",
    "# Model with pretrained weights\n",
    "cnn_lstm_with_ae = CNNLSTMOnAutoencoder(trained_autoencoder, hidden_size=128, num_classes=2).to(device)\n",
    "\n",
    "# Optionally, freeze the encoder layers, Frozen= false, Unfrozen= true\n",
    "for param in cnn_lstm_with_ae.encoder.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = torch.optim.SGD(cnn_lstm_with_ae.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KfevQjv7erK1"
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "\n",
    "num_epochs = 100\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    cnn_lstm_with_ae.train()\n",
    "    total_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        # Send inputs and labels to the device (GPU if available)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output = cnn_lstm_with_ae(inputs)\n",
    "\n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate the loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Store predictions and labels for accuracy calculation\n",
    "        _, preds = torch.max(output, 1)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    # Calculate training accuracy and average loss for the epoch\n",
    "    train_accuracy = accuracy_score(all_labels, all_preds)\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}')\n",
    "\n",
    "    # Validation phase\n",
    "    cnn_lstm_with_ae.eval()\n",
    "    val_loss = 0.0\n",
    "    val_labels = []\n",
    "    val_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_labels_batch in val_loader:\n",
    "            val_inputs = val_inputs.to(device)\n",
    "            val_labels_batch = val_labels_batch.to(device)\n",
    "\n",
    "            # Forward pass for validation data\n",
    "            val_outputs = cnn_lstm_with_ae(val_inputs)\n",
    "            loss = criterion(val_outputs, val_labels_batch)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Store predictions and labels for accuracy calculation\n",
    "            _, val_preds_batch = torch.max(val_outputs, 1)\n",
    "            val_labels.extend(val_labels_batch.cpu().numpy())\n",
    "            val_preds.extend(val_preds_batch.cpu().numpy())\n",
    "\n",
    "    # Calculate validation accuracy and average loss\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = accuracy_score(val_labels, val_preds)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "    # Save the model if validation loss improves\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(cnn_lstm_with_ae.state_dict(), 'best_cnnlstm_with_ae.pt')\n",
    "        print(f'Model saved at epoch {epoch+1} with validation loss: {avg_val_loss:.4f}')\n",
    "\n",
    "    # Save the model after each epoch\n",
    "    torch.save(cnn_lstm_with_ae.state_dict(), f'cnnlstm_with_ae_epoch{epoch+1}.pt')\n",
    "\n",
    "# Save the final trained model\n",
    "torch.save(cnn_lstm_with_ae.state_dict(), 'cnnlstm_with_ae_final.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oZppsrq1eun2"
   },
   "outputs": [],
   "source": [
    "# Testing loop\n",
    "\n",
    "# Load trained autoencoder\n",
    "trained_autoencoder = CNN_Autoencoder().to(device)\n",
    "trained_autoencoder.load_state_dict(torch.load(save_path, weights_only=True))\n",
    "trained_autoencoder.eval() # Set the model to evaluation mode\n",
    "\n",
    "# Load the model\n",
    "classifier_model = CNNLSTMOnAutoencoder(trained_autoencoder, hidden_size=128, num_classes=2).to(device)  # Initialize your classifier model\n",
    "model_state_dict = torch.load('cnnlstm_with_ae_final.pt', weights_only=False)\n",
    "classifier_model.load_state_dict(model_state_dict)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "classifier_model.eval()\n",
    "\n",
    "# Define criterion (loss function)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Test the model\n",
    "correct = 0\n",
    "total = 0\n",
    "total_loss = 0.0\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        # Send inputs and labels to the device (GPU if available)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output = simple_cnn_model(inputs)\n",
    "        # print(outputs.shape)\n",
    "\n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Store predictions and labels for confusion matrix\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Calculate accuracy and average loss\n",
    "accuracy = 100 * correct / total\n",
    "average_loss = total_loss / len(test_loader)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# Plot confusion matrix using seaborn\n",
    "fig, ax = plt.subplots(figsize=(18, 16))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax,\n",
    "            xticklabels=[f'Class {i}' for i in range(num_classes)],\n",
    "            yticklabels=[f'Class {i}' for i in range(num_classes)])\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('True')\n",
    "ax.set_title('Confusion Matrix')\n",
    "\n",
    "# Print accuracy and average loss\n",
    "print(f'Accuracy of the CNN-LSTM with autoencoder on the test images: {accuracy:.2f}%')\n",
    "print(f'Average loss on the test images: {average_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XtCf0ayFa-f5"
   },
   "source": [
    "### Not using pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "otTTcIpabGSF"
   },
   "outputs": [],
   "source": [
    "class SimpleCNNLSTM(nn.Module):\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super(SimpleCNNLSTM, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv3d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv3d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.AdaptiveMaxPool3d(output_size=(8, 8, 5))\n",
    "        self.lstm = None # To be defined later after determining input size\n",
    "        self.fc = None  # To be defined later after determining LSTM output size\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Apply convolutions, dropout, and pooling layers\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        # print(f'First conv + pooling {x.shape}')\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        # print(f'Second conv + pooling {x.shape}')\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        # print(f'Third conv + pooling {x.shape}')\n",
    "\n",
    "        # Get the final shape after convolutions\n",
    "        conv_output_shape = x.shape\n",
    "        # print(f\"Convolution output shape: {conv_output_shape}\")\n",
    "        \n",
    "        # Extract available dimensions for reshaping\n",
    "        batch_size, channels, new_height, new_width, new_depth = conv_output_shape\n",
    "\n",
    "        # Calculate the LSTM input size\n",
    "        lstm_input_size = channels * new_height * new_width\n",
    "\n",
    "        # Reshape for LSTM input\n",
    "        x = x.view(batch_size, new_depth, lstm_input_size)\n",
    "\n",
    "        # Initialize LSTM and FC layers if not yet initialized\n",
    "        if self.lstm is None:\n",
    "            self.lstm = nn.LSTM(input_size=lstm_input_size, hidden_size=128, batch_first=True).to(x.device)\n",
    "            self.fc = nn.Linear(in_features=128, out_features=2).to(x.device)\n",
    "\n",
    "        # Pass through LSTM and FC\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        out = F.relu(self.fc(lstm_out[:, -1, :]))\n",
    "        out = F.softmax(out, dim=1)  # Apply softmax for probabilistic output\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model without pretrained weights\n",
    "simple_cnnlstm = SimpleCNNLSTM(hidden_size=128, num_classes=2).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = torch.optim.SGD(simple_cnnlstm.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "Gcu0LbVJc65y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Train Loss: 0.6769, Train Accuracy: 0.6246\n",
      "Epoch [1/100], Val Loss: 0.6737, Val Accuracy: 0.6248\n",
      "Model saved at epoch 1 with validation loss: 0.6737\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m all_labels \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     11\u001b[0m all_preds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (inputs, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# Send inputs and labels to the device (GPU if available)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# Zero the gradients\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[64], line 25\u001b[0m, in \u001b[0;36mFMRI_Dataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     22\u001b[0m data \u001b[38;5;241m=\u001b[39m fmri_img\u001b[38;5;241m.\u001b[39mget_fdata()\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Apply Gaussian smoothing\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msmooth_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Apply augmentations if enabled\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maugment:\n",
      "Cell \u001b[0;32mIn[64], line 80\u001b[0m, in \u001b[0;36mFMRI_Dataset.smooth_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msmooth_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mndimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgaussian_filter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msmoothing_sigma\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/scipy/ndimage/_filters.py:368\u001b[0m, in \u001b[0;36mgaussian_filter\u001b[0;34m(input, sigma, order, output, mode, cval, truncate, radius)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(axes) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    367\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m axis, sigma, order, mode, radius \u001b[38;5;129;01min\u001b[39;00m axes:\n\u001b[0;32m--> 368\u001b[0m         \u001b[43mgaussian_filter1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mradius\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mradius\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    370\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m output\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/scipy/ndimage/_filters.py:276\u001b[0m, in \u001b[0;36mgaussian_filter1d\u001b[0;34m(input, sigma, axis, order, output, mode, cval, truncate, radius)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;66;03m# Since we are calling correlate, not convolve, revert the kernel\u001b[39;00m\n\u001b[1;32m    275\u001b[0m weights \u001b[38;5;241m=\u001b[39m _gaussian_kernel1d(sigma, order, lw)[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 276\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcorrelate1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/scipy/ndimage/_filters.py:134\u001b[0m, in \u001b[0;36mcorrelate1d\u001b[0;34m(input, weights, axis, output, mode, cval, origin)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInvalid origin; origin must satisfy \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    131\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-(len(weights) // 2) <= origin <= \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    132\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(len(weights)-1) // 2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    133\u001b[0m mode \u001b[38;5;241m=\u001b[39m _ni_support\u001b[38;5;241m.\u001b[39m_extend_mode_to_code(mode)\n\u001b[0;32m--> 134\u001b[0m \u001b[43m_nd_image\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorrelate1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m                      \u001b[49m\u001b[43morigin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "\n",
    "num_epochs = 100\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    simple_cnnlstm.train()\n",
    "    total_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        # Send inputs and labels to the device (GPU if available)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output = simple_cnnlstm(inputs)\n",
    "\n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate the loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Store predictions and labels for accuracy calculation\n",
    "        _, preds = torch.max(output, 1)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    # Calculate training accuracy and average loss for the epoch\n",
    "    train_accuracy = accuracy_score(all_labels, all_preds)\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}')\n",
    "\n",
    "    # Validation phase\n",
    "    simple_cnnlstm.eval()\n",
    "    val_loss = 0.0\n",
    "    val_labels = []\n",
    "    val_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_labels_batch in val_loader:\n",
    "            val_inputs = val_inputs.to(device)\n",
    "            val_labels_batch = val_labels_batch.to(device)\n",
    "\n",
    "            # Forward pass for validation data\n",
    "            val_outputs = simple_cnnlstm(val_inputs)\n",
    "            loss = criterion(val_outputs, val_labels_batch)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Store predictions and labels for accuracy calculation\n",
    "            _, val_preds_batch = torch.max(val_outputs, 1)\n",
    "            val_labels.extend(val_labels_batch.cpu().numpy())\n",
    "            val_preds.extend(val_preds_batch.cpu().numpy())\n",
    "\n",
    "    # Calculate validation accuracy and average loss\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = accuracy_score(val_labels, val_preds)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "    # Save the model if validation loss improves\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(simple_cnnlstm.state_dict(), 'best_simple_cnnlstm.pt')\n",
    "        print(f'Model saved at epoch {epoch+1} with validation loss: {avg_val_loss:.4f}')\n",
    "\n",
    "    # Save the model after each epoch\n",
    "    torch.save(simple_cnnlstm.state_dict(), f'simple_cnnlstm_epoch{epoch+1}.pt')\n",
    "\n",
    "# Save the final trained model\n",
    "torch.save(simple_cnnlstm.state_dict(), 'simple_cnnlstm_final.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8O9R_idQdIef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the simple CNN-LSTM on the test images: 38.69%\n",
      "Average loss on the test images: 0.7741\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x1600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Testing loop\n",
    "\n",
    "# Load the model\n",
    "classifier_model = SimpleCNNLSTM(hidden_size=128, num_classes=2).to(device)  # Initialize your classifier model\n",
    "model_state_dict = torch.load('simple_cnnlstm_final.pt', weights_only=False)\n",
    "classifier_model.load_state_dict(model_state_dict)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "classifier_model.eval()\n",
    "\n",
    "# Define criterion (loss function)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Test the model\n",
    "correct = 0\n",
    "total = 0\n",
    "total_loss = 0.0\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        # Send inputs and labels to the device (GPU if available)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output = simple_cnn_model(inputs)\n",
    "        # print(outputs.shape)\n",
    "\n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Store predictions and labels for confusion matrix\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Calculate accuracy and average loss\n",
    "accuracy = 100 * correct / total\n",
    "average_loss = total_loss / len(test_loader)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# Plot confusion matrix using seaborn\n",
    "fig, ax = plt.subplots(figsize=(18, 16))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax,\n",
    "            xticklabels=[f'Class {i}' for i in range(num_classes)],\n",
    "            yticklabels=[f'Class {i}' for i in range(num_classes)])\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('True')\n",
    "ax.set_title('Confusion Matrix')\n",
    "\n",
    "# Print accuracy and average loss\n",
    "print(f'Accuracy of the simple CNN-LSTM on the test images: {accuracy:.2f}%')\n",
    "print(f'Average loss on the test images: {average_loss:.4f}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "S-BF1plyaGTM",
    "EhOKd9tEVyhT",
    "XtCf0ayFa-f5"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
