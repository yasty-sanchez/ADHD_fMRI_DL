{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "GWvNv3IrQtwK"
   },
   "outputs": [],
   "source": [
    "# Torch-related imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "\n",
    "# Scikit-learn-related imports\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# Nibabel and Scipy imports (for handling fMRI and image processing)\n",
    "import nibabel as nib\n",
    "import scipy.ndimage as ndimage  # For smoothing\n",
    "\n",
    "# NumPy, Matplotlib, and Seaborn (for data manipulation and visualization)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# OS for file system operations\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 219,
     "status": "ok",
     "timestamp": 1729905899637,
     "user": {
      "displayName": "Yasty Sánchez",
      "userId": "01336246420740504937"
     },
     "user_tz": 360
    },
    "id": "PBnx2TrBRAvy",
    "outputId": "7abe6154-a5d7-4cdc-b770-78d6db615511"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '0'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3693,
     "status": "ok",
     "timestamp": 1729906065409,
     "user": {
      "displayName": "Yasty Sánchez",
      "userId": "01336246420740504937"
     },
     "user_tz": 360
    },
    "id": "ELtPOdZYQ9ny",
    "outputId": "2975f1d9-50c4-42d0-ca16-e5c337366126"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# root_dir = os.path.join('/content/drive', 'My Drive', 'UCR', '2-2024', 'InvCC', 'ADHD200', 'Datasets', 'preprocessed')\n",
    "\n",
    "root_dir = os.path.join('preprocessed')\n",
    "\n",
    "# Carpetas para TDC y ADHD\n",
    "tdc_dir = os.path.join(root_dir, 'TDC')\n",
    "adhd_dir = os.path.join(root_dir, 'ADHD')\n",
    "\n",
    "# Para guardar el estado del autoencoder\n",
    "save_path = os.path.join(root_dir, 'autoencoder.pt')\n",
    "\n",
    "# Listas para almacenar las rutas de archivos\n",
    "tdc_file_paths = [os.path.join(tdc_dir, file) for file in os.listdir(tdc_dir) if file.endswith('.nii.gz')]\n",
    "adhd_file_paths = [os.path.join(adhd_dir, file) for file in os.listdir(adhd_dir) if file.endswith('.nii.gz')]\n",
    "\n",
    "# Etiquetas correspondientes\n",
    "tdc_labels = [0] * len(tdc_file_paths)\n",
    "adhd_labels = [1] * len(adhd_file_paths)\n",
    "\n",
    "# Combinar rutas de archivos y etiquetas\n",
    "file_paths = tdc_file_paths + adhd_file_paths\n",
    "labels = tdc_labels + adhd_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rzMlmaGzUNdd"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "_6Oj7oNrUHLO"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import nibabel as nib\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "\n",
    "class FMRI_Dataset(Dataset):\n",
    "    def __init__(self, file_paths, labels, max_shape, smoothing_sigma=1, augment=False):\n",
    "        self.file_paths = file_paths  # List of paths to the fMRI data files\n",
    "        self.labels = labels  # Corresponding labels\n",
    "        self.max_shape = max_shape  # Shape to pad all inputs to (e.g., [1, 53, 64, 46, 512])\n",
    "        self.smoothing_sigma = smoothing_sigma  # Standard deviation for Gaussian smoothing\n",
    "        self.augment = augment  # Apply augmentations if True\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load fMRI data using NiBabel\n",
    "        fmri_img = nib.load(self.file_paths[idx])\n",
    "        data = fmri_img.get_fdata()  # Extract the fMRI data as a NumPy array\n",
    "\n",
    "        # Apply Gaussian smoothing\n",
    "        data = self.smooth_data(data)\n",
    "\n",
    "        # Apply augmentations if enabled\n",
    "        if self.augment:\n",
    "            data = self.apply_augmentations(data)\n",
    "\n",
    "        # Normalize the data\n",
    "        data = self.normalize_data(data)\n",
    "\n",
    "        # Convert to tensor and add missing dimensions\n",
    "        data_tensor = torch.tensor(data, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        # Pad the tensor to the specified max_shape\n",
    "        data_padded = F.pad(data_tensor, pad=self.calculate_padding(data_tensor.shape), mode='constant', value=0)\n",
    "\n",
    "        # Ensure that the final shape matches max_shape exactly\n",
    "        data_padded = data_padded.view(*self.max_shape)\n",
    "\n",
    "        # Get the label\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "        return data_padded, label\n",
    "\n",
    "    def apply_augmentations(self, data):\n",
    "        # Add Gaussian noise\n",
    "        data = self.add_noise(data, mean=0, std=0.01)\n",
    "\n",
    "        # Apply random rotation\n",
    "        data = self.random_rotate(data)\n",
    "\n",
    "        # Apply random intensity shift\n",
    "        data = self.random_intensity_shift(data, shift_limit=0.05)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def add_noise(self, data, mean=0, std=0.01):\n",
    "        noise = np.random.normal(mean, std, data.shape)\n",
    "        return data + noise\n",
    "\n",
    "    def random_rotate(self, data):\n",
    "        angles = np.random.uniform(-5, 5, size=3)  # Small random rotation angles for x, y, z axes\n",
    "        return ndimage.rotate(data, angle=angles[0], axes=(1, 2), reshape=False, mode='nearest')\n",
    "\n",
    "    def random_intensity_shift(self, data, shift_limit=0.05):\n",
    "        shift_value = np.random.uniform(-shift_limit, shift_limit)\n",
    "        return data + shift_value\n",
    "\n",
    "    def calculate_padding(self, current_shape):\n",
    "        padding = []\n",
    "        for current_dim, max_dim in zip(reversed(current_shape), reversed(self.max_shape)):\n",
    "            pad_total = max_dim - current_dim\n",
    "            padding.append(pad_total // 2)  # pad_left or pad_top\n",
    "            padding.append(pad_total - (pad_total // 2))  # pad_right or pad_bottom\n",
    "        return padding\n",
    "\n",
    "    def normalize_data(self, data):\n",
    "        \"\"\"Normalize the data to zero mean and unit variance.\"\"\"\n",
    "        mean = data.mean()\n",
    "        std = data.std()\n",
    "        if std > 0:  # Avoid division by zero\n",
    "            data = (data - mean) / std\n",
    "        return data\n",
    "\n",
    "    def smooth_data(self, data):\n",
    "        \"\"\"Apply Gaussian smoothing to the data.\"\"\"\n",
    "        return ndimage.gaussian_filter(data, sigma=self.smoothing_sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "n8wMvOSlXa0N",
    "outputId": "17ebcb4d-69c9-4232-8cfc-980c8e921933"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution: Counter({1: 1462, 0: 1340})\n",
      "torch.Size([1, 53, 64, 46, 512]) tensor(0)\n"
     ]
    }
   ],
   "source": [
    "# Determine the maximum shape across all tensors\n",
    "max_shape = [1, 53, 64, 46, 512]\n",
    "\n",
    "# Create the dataset with padded tensors\n",
    "from collections import Counter\n",
    "\n",
    "# Assuming you have an instance of your FMRI_Dataset\n",
    "dataset = FMRI_Dataset(file_paths, labels, max_shape)\n",
    "\n",
    "# Count the labels\n",
    "label_counts = Counter(dataset.labels)\n",
    "print(\"Class distribution:\", label_counts)\n",
    "\n",
    "sample_data, sample_label = dataset[0]\n",
    "print(sample_data.shape, sample_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "0a9aet2DXa0O"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Shape of inputs: torch.Size([4, 1, 53, 64, 46, 512]), Shape of labels: torch.Size([4])\n",
      "Test - Shape of inputs: torch.Size([4, 1, 53, 64, 46, 512]), Shape of labels: torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import torch\n",
    "\n",
    "# Assuming `dataset` is your instantiated FMRI_Dataset class with labels.\n",
    "\n",
    "# Create a StratifiedShuffleSplit object to split into training and test sets\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "# Ensure labels are on CPU for compatibility with sklearn\n",
    "labels_cpu = dataset.labels.cpu().numpy() if isinstance(dataset.labels, torch.Tensor) else dataset.labels\n",
    "\n",
    "# Split into training/validation and test sets\n",
    "for train_val_idx, test_idx in sss.split(range(len(dataset)), labels_cpu):\n",
    "    train_val_set = Subset(dataset, train_val_idx)\n",
    "    testset = Subset(dataset, test_idx)\n",
    "\n",
    "# Create another StratifiedShuffleSplit for training and validation sets\n",
    "sss_val = StratifiedShuffleSplit(n_splits=1, test_size=0.25, random_state=42)  # 0.25 of 0.8 gives 0.2 total for validation\n",
    "\n",
    "# Access labels for train_val_set\n",
    "train_val_labels = [labels_cpu[i] for i in train_val_idx]  # Use list comprehension to retrieve labels\n",
    "\n",
    "# Split train_val_set into train and validation sets\n",
    "for train_idx, val_idx in sss_val.split(range(len(train_val_set)), train_val_labels):\n",
    "    trainset = Subset(train_val_set, train_idx)\n",
    "    valset = Subset(train_val_set, val_idx)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 4\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "valloader = DataLoader(valset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "class_names = [\"TDC\", \"ADHD\"]\n",
    "\n",
    "# Verify the dataloaders\n",
    "# Print shapes of a sample batch from the training loader\n",
    "for images, labels in trainloader:\n",
    "    if images is not None:\n",
    "        print(f\"Train - Shape of inputs: {images.shape}, Shape of labels: {labels.shape}\")\n",
    "        break\n",
    "\n",
    "# Print shapes of a sample batch from the test loader\n",
    "for images, labels in testloader:\n",
    "    if images is not None:\n",
    "        print(f\"Test - Shape of inputs: {images.shape}, Shape of labels: {labels.shape}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "wYVevU2sXa0O",
    "outputId": "9a880f96-c9f5-460c-f121-cec37986aa3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in valloader: 561\n",
      "Number of samples in trainloader: 1680\n",
      "Number of samples in testloader: 561\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of samples in valloader: {len(valloader.dataset)}\")\n",
    "print(f\"Number of samples in trainloader: {len(trainloader.dataset)}\")\n",
    "print(f\"Number of samples in testloader: {len(testloader.dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN-AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 53, 64, 46])\n"
     ]
    }
   ],
   "source": [
    "class CNN_Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_Autoencoder, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv3d(1, 16, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(16, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose3d(256, 128, kernel_size=3, stride=2, padding=(1, 1, 1), output_padding=(0, 1, 0)),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose3d(128, 64, kernel_size=3, stride=2, padding=(1, 1, 1), output_padding=(1, 0, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose3d(64, 16, kernel_size=3, stride=2, padding=(1, 0, 0), output_padding=(0, 0, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose3d(16, 1, kernel_size=3, stride=2, padding=(1, 0, 0), output_padding=(0, 1, 1)),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "autoencoder = CNN_Autoencoder().to(device)\n",
    "\n",
    "# Generate random input matching new shape [1, 1, 53, 64] (Batch size 1)\n",
    "inputs = torch.rand((1, 53, 64, 46)).to(device)  # Example input\n",
    "output = autoencoder(inputs)\n",
    "print(output.shape)  # should match the input shape [1, 53, 64, 46]\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()  # Since it's an autoencoder, Mean Squared Error is commonly used\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=0.001, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EYw0MT13SblI"
   },
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S-BF1plyaGTM"
   },
   "source": [
    "### Using pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "HR232TTAWkLH"
   },
   "outputs": [],
   "source": [
    "# Model definition\n",
    "\n",
    "class CNNOnEncoder(nn.Module):\n",
    "    def __init__(self, autoencoder, num_classes):\n",
    "        super(CNNOnEncoder, self).__init__()\n",
    "        self.encoder = autoencoder.encoder  # Use the encoder from the autoencoder\n",
    "        self.conv1 = nn.Conv3d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv3d(in_channels=512, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv3d(in_channels=256, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.AdaptiveMaxPool3d(output_size=(8, 8, 5))\n",
    "        self.fc1 = nn.Linear(128 * 8 * 8 * 5, 256)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        x = self.encoder(x)\n",
    "        # print(f'Encoder {x.shape}')\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # First conv + pooling\n",
    "        # print(f'First conv + pooling {x.shape}')\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # Second conv + pooling\n",
    "        # print(f'Second conv + pooling {x.shape}')\n",
    "        x = self.pool(F.relu(self.conv3(x)))  # Second conv + pooling\n",
    "        x = x.unsqueeze(0)\n",
    "        x = x.view(x.size(1), -1)  # Flatten\n",
    "        # print(f'Flattened {x.shape}')\n",
    "        x = F.relu(self.fc1(x))  # First fully connected layer\n",
    "        x = self.fc2(x)  # Output layer\n",
    "        x = F.softmax(x, dim=1)  # Apply softmax for probabilistic output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "rl5qWwF7aAIi"
   },
   "outputs": [],
   "source": [
    "# Load trained autoencoder\n",
    "trained_autoencoder = CNN_Autoencoder()\n",
    "trained_autoencoder.load_state_dict(torch.load(save_path, weights_only=True))\n",
    "trained_autoencoder.to(device)\n",
    "trained_autoencoder.eval() # Set the model to evaluation mode\n",
    "\n",
    "# Create an instance of the new model\n",
    "cnn_with_ae_model = CNNOnEncoder(trained_autoencoder, num_classes=2).to(device)\n",
    "\n",
    "# Optionally, freeze the encoder layers, Frozen= false, Unfrozen= true\n",
    "for param in cnn_with_ae_model.encoder.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "class_weights = torch.tensor([1.0, 1.1])\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = torch.optim.SGD(cnn_with_ae_model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 216
    },
    "executionInfo": {
     "elapsed": 255,
     "status": "error",
     "timestamp": 1725738817442,
     "user": {
      "displayName": "Yasty Sánchez",
      "userId": "01336246420740504937"
     },
     "user_tz": 360
    },
    "id": "JtGhl4d5WuQ8",
    "outputId": "8ce13504-e388-4ebe-f475-51f24117dc37"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[116], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mcnn_with_ae_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, labels)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[114], line 16\u001b[0m, in \u001b[0;36mCNNOnEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# print(x.shape)\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# print(f'Encoder {x.shape}')\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)))  \u001b[38;5;66;03m# First conv + pooling\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:608\u001b[0m, in \u001b[0;36mConv3d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:603\u001b[0m, in \u001b[0;36mConv3d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    592\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv3d(\n\u001b[1;32m    593\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    594\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    602\u001b[0m     )\n\u001b[0;32m--> 603\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv3d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    cnn_with_ae_model.train()\n",
    "    total_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    for batch_idx, (inputs, labels) in enumerate(trainloader):\n",
    "        inputs_reduced = torch.mean(inputs, dim=-1)\n",
    "        inputs = inputs_reduced\n",
    "\n",
    "        # Send inputs and labels to the device (GPU if available)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output = cnn_with_ae_model(inputs)\n",
    "\n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate the loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Store predictions and labels for accuracy calculation\n",
    "        _, preds = torch.max(output, 1)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    # Calculate training accuracy and average loss for the epoch\n",
    "    train_accuracy = accuracy_score(all_labels, all_preds)\n",
    "    avg_train_loss = total_loss / len(trainloader)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}')\n",
    "\n",
    "    # Validation phase\n",
    "    cnn_with_ae_model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_labels = []\n",
    "    val_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_labels_batch in valloader:\n",
    "            val_inputs_reduced = torch.mean(val_inputs, dim=-1)\n",
    "            val_inputs = val_inputs_reduced.to(device)\n",
    "            val_labels_batch = val_labels_batch.to(device)\n",
    "\n",
    "            # Forward pass for validation data\n",
    "            val_outputs = cnn_with_ae_model(val_inputs)\n",
    "            loss = criterion(val_outputs, val_labels_batch)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Store predictions and labels for accuracy calculation\n",
    "            _, val_preds_batch = torch.max(val_outputs, 1)\n",
    "            val_labels.extend(val_labels_batch.cpu().numpy())\n",
    "            val_preds.extend(val_preds_batch.cpu().numpy())\n",
    "\n",
    "    # Calculate validation accuracy and average loss\n",
    "    avg_val_loss = val_loss / len(valloader)\n",
    "    val_accuracy = accuracy_score(val_labels, val_preds)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "    # Save the model if validation loss improves\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(cnn_with_ae_model.state_dict(), 'best_cnn_with_ae.pt')\n",
    "        print(f'Model saved at epoch {epoch+1} with validation loss: {avg_val_loss:.4f}')\n",
    "\n",
    "    # Save the model after each epoch\n",
    "    torch.save(cnn_with_ae_model.state_dict(), f'cnn_with_ae_epoch{epoch}.pt')\n",
    "\n",
    "# Save the final trained model\n",
    "torch.save(cnn_with_ae_model.state_dict(), 'cnn_with_ae_final.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "NC_sALTGWwog"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the CNN with autoencoder on the test images: 52.23%\n",
      "Average loss on the test images: 0.6923\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgUAAAIjCAYAAACTaWgmAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABE1UlEQVR4nO3dd3wUdf7H8fcmIUuAVEoKJfQSqhQxgpQjR0eaSgQhICB6iS00UbpoPOQEC8IVBQ7BgigqKtIURAJyFCkiEAhyHiT0REoSSOb3B7I/h4AkwGYS5vX0MY8HmZmd/ey6yifv7/c76zAMwxAAALA9D6sLAAAAhQNNAQAAkERTAAAAfkNTAAAAJNEUAACA39AUAAAASTQFAADgNzQFAABAEk0BAAD4DU0BkEf79u1T+/bt5e/vL4fDoSVLltzS6x88eFAOh0Nz5869pdctytq0aaM2bdpYXQZgGzQFKFL279+vYcOGqWrVqipevLj8/PzUokULvfrqqzp//rxbnzsmJkY7duzQCy+8oPnz56tp06Zufb6CNHDgQDkcDvn5+V31fdy3b58cDoccDoemTZuW7+sfPnxYEydO1LZt225BtQDcxcvqAoC8+vzzz3X//ffL6XRqwIABqlevnrKysrRu3TqNHDlSu3bt0j/+8Q+3PPf58+eVmJio5557TnFxcW55jvDwcJ0/f17FihVzy/Wvx8vLS+fOndNnn32mBx54wHRswYIFKl68uDIyMm7o2ocPH9akSZNUuXJlNWrUKM+PW758+Q09H4AbQ1OAIiE5OVnR0dEKDw/X6tWrFRoa6joWGxurpKQkff755257/mPHjkmSAgIC3PYcDodDxYsXd9v1r8fpdKpFixZ69913czUFCxcuVJcuXbR48eICqeXcuXMqUaKEvL29C+T5AFzC8AGKhKlTp+rMmTN66623TA3BZdWrV9eTTz7p+vnixYt6/vnnVa1aNTmdTlWuXFnPPvusMjMzTY+rXLmyunbtqnXr1unOO+9U8eLFVbVqVf373/92nTNx4kSFh4dLkkaOHCmHw6HKlStLuhS7X/7z702cOFEOh8O0b8WKFWrZsqUCAgJUqlQp1apVS88++6zr+LXmFKxevVr33HOPSpYsqYCAAHXv3l27d+++6vMlJSVp4MCBCggIkL+/vwYNGqRz585d+429Qt++ffXll1/q9OnTrn2bNm3Svn371Ldv31znnzx5UiNGjFD9+vVVqlQp+fn5qVOnTvrhhx9c53zzzTdq1qyZJGnQoEGuYYjLr7NNmzaqV6+eNm/erFatWqlEiRKu9+XKOQUxMTEqXrx4rtffoUMHBQYG6vDhw3l+rQByoylAkfDZZ5+patWquvvuu/N0/pAhQzR+/Hg1btxY06dPV+vWrZWQkKDo6Ohc5yYlJem+++7Tn//8Z/3tb39TYGCgBg4cqF27dkmSevXqpenTp0uSHnzwQc2fP18zZszIV/27du1S165dlZmZqcmTJ+tvf/ub7r33Xn333Xd/+LiVK1eqQ4cOOnr0qCZOnKj4+HitX79eLVq00MGDB3Od/8ADD+jXX39VQkKCHnjgAc2dO1eTJk3Kc529evWSw+HQRx995Nq3cOFC1a5dW40bN851/oEDB7RkyRJ17dpVr7zyikaOHKkdO3aodevWrr+g69Spo8mTJ0uSHnnkEc2fP1/z589Xq1atXNc5ceKEOnXqpEaNGmnGjBlq27btVet79dVXVbZsWcXExCg7O1uS9Pe//13Lly/X66+/rrCwsDy/VgBXYQCFXFpamiHJ6N69e57O37ZtmyHJGDJkiGn/iBEjDEnG6tWrXfvCw8MNScbatWtd+44ePWo4nU5j+PDhrn3JycmGJOPll182XTMmJsYIDw/PVcOECROM3//nNX36dEOScezYsWvWffk55syZ49rXqFEjo1y5csaJEydc+3744QfDw8PDGDBgQK7ne/jhh03X7Nmzp1G6dOlrPufvX0fJkiUNwzCM++67z2jXrp1hGIaRnZ1thISEGJMmTbrqe5CRkWFkZ2fneh1Op9OYPHmya9+mTZtyvbbLWrdubUgyZs+efdVjrVu3Nu376quvDEnGlClTjAMHDhilSpUyevTocd3XCOD6SApQ6KWnp0uSfH1983T+F198IUmKj4837R8+fLgk5Zp7EBERoXvuucf1c9myZVWrVi0dOHDghmu+0uW5CJ988olycnLy9JgjR45o27ZtGjhwoIKCglz7GzRooD//+c+u1/l7jz76qOnne+65RydOnHC9h3nRt29fffPNN0pJSdHq1auVkpJy1aED6dI8BA+PS/8byc7O1okTJ1xDI1u2bMnzczqdTg0aNChP57Zv317Dhg3T5MmT1atXLxUvXlx///vf8/xcAK6NpgCFnp+fnyTp119/zdP5P//8szw8PFS9enXT/pCQEAUEBOjnn3827a9UqVKuawQGBurUqVM3WHFuffr0UYsWLTRkyBAFBwcrOjpaH3zwwR82CJfrrFWrVq5jderU0fHjx3X27FnT/itfS2BgoCTl67V07txZvr6+ev/997VgwQI1a9Ys13t5WU5OjqZPn64aNWrI6XSqTJkyKlu2rLZv3660tLQ8P2f58uXzNalw2rRpCgoK0rZt2/Taa6+pXLlyeX4sgGujKUCh5+fnp7CwMO3cuTNfj7tyot+1eHp6XnW/YRg3/ByXx7sv8/Hx0dq1a7Vy5Ur1799f27dvV58+ffTnP/8517k342Zey2VOp1O9evXSvHnz9PHHH18zJZCkF198UfHx8WrVqpXeeecdffXVV1qxYoXq1q2b50REuvT+5MfWrVt19OhRSdKOHTvy9VgA10ZTgCKha9eu2r9/vxITE697bnh4uHJycrRv3z7T/tTUVJ0+fdq1kuBWCAwMNM3Uv+zKNEKSPDw81K5dO73yyiv68ccf9cILL2j16tX6+uuvr3rty3Xu2bMn17GffvpJZcqUUcmSJW/uBVxD3759tXXrVv36669XnZx52Ycffqi2bdvqrbfeUnR0tNq3b6+oqKhc70leG7S8OHv2rAYNGqSIiAg98sgjmjp1qjZt2nTLrg/YGU0BioRRo0apZMmSGjJkiFJTU3Md379/v1599VVJl+JvSblWCLzyyiuSpC5dutyyuqpVq6a0tDRt377dte/IkSP6+OOPTeedPHky12Mv38TnymWSl4WGhqpRo0aaN2+e6S/ZnTt3avny5a7X6Q5t27bV888/rzfeeEMhISHXPM/T0zNXCrFo0SL973//M+273LxcrYHKr9GjR+vQoUOaN2+eXnnlFVWuXFkxMTHXfB8B5B03L0KRUK1aNS1cuFB9+vRRnTp1THc0XL9+vRYtWqSBAwdKkho2bKiYmBj94x//0OnTp9W6dWt9//33mjdvnnr06HHN5W43Ijo6WqNHj1bPnj31xBNP6Ny5c5o1a5Zq1qxpmmg3efJkrV27Vl26dFF4eLiOHj2qN998UxUqVFDLli2vef2XX35ZnTp1UmRkpAYPHqzz58/r9ddfl7+/vyZOnHjLXseVPDw8NHbs2Oue17VrV02ePFmDBg3S3XffrR07dmjBggWqWrWq6bxq1aopICBAs2fPlq+vr0qWLKnmzZurSpUq+apr9erVevPNNzVhwgTXEsk5c+aoTZs2GjdunKZOnZqv6wG4gsWrH4B82bt3rzF06FCjcuXKhre3t+Hr62u0aNHCeP31142MjAzXeRcuXDAmTZpkVKlSxShWrJhRsWJFY8yYMaZzDOPSksQuXbrkep4rl8Jda0miYRjG8uXLjXr16hne3t5GrVq1jHfeeSfXksRVq1YZ3bt3N8LCwgxvb28jLCzMePDBB429e/fmeo4rl+2tXLnSaNGiheHj42P4+fkZ3bp1M3788UfTOZef78olj3PmzDEkGcnJydd8Tw3DvCTxWq61JHH48OFGaGio4ePjY7Ro0cJITEy86lLCTz75xIiIiDC8vLxMr7N169ZG3bp1r/qcv79Oenq6ER4ebjRu3Ni4cOGC6bynn37a8PDwMBITE//wNQD4Yw7DyMcMJAAAcNtiTgEAAJBEUwAAAH5DUwAAACTRFAAAgN/QFAAAAEk0BQAA4Dc0BQAAQNJtekfDjItWVwC4X2CXaVaXALjd+a9GuPX6PnfEue3a57e+4bZruwtJAQAAkHSbJgUAAOSJg9+Nf4+mAABgX7fwa71vB7RIAABAEkkBAMDOGD4w4d0AAACSSAoAAHbGnAITkgIAACCJpAAAYGfMKTDh3QAAAJJICgAAdsacAhOaAgCAfTF8YMK7AQAAJJEUAADsjOEDE5ICAAAgiaQAAGBnzCkw4d0AAACSSAoAAHbGnAITkgIAACCJpAAAYGfMKTChKQAA2BfDBya0SAAAQBJJAQDAzhg+MOHdAAAAkkgKAAB2RlJgwrsBAAAkkRQAAOzMg9UHv0dSAAAAJJEUAADsjDkFJjQFAAD74uZFJrRIAABAEkkBAMDOGD4w4d0AAACSSAoAAHbGnAITkgIAACCJpAAAYGfMKTDh3QAAAJJICgAAdsacAhOaAgCAfTF8YMK7AQAAJJEUAADsjOEDE5ICAAAgiaQAAGBnzCkw4d0AAACSSAoAAHbGnAITkgIAACCJpAAAYGfMKTChKQAA2BdNgQnvBgAAkERSAACwMyYampAUAAAASSQFAAA7Y06BCe8GAACQRFIAALAz5hSYkBQAAABJJAUAADtjToEJTQEAwL4YPjChRQIAAJJICgAANuYgKTAhKQAAAJJICgAANkZSYEZSAAAAJJEUAADsjKDAhKQAAAALJSQkqFmzZvL19VW5cuXUo0cP7dmzx3ROmzZt5HA4TNujjz5qOufQoUPq0qWLSpQooXLlymnkyJG6ePFivmohKQAA2FZhmFOwZs0axcbGqlmzZrp48aKeffZZtW/fXj/++KNKlizpOm/o0KGaPHmy6+cSJUq4/pydna0uXbooJCRE69ev15EjRzRgwAAVK1ZML774Yp5roSkAANhWYWgKli1bZvp57ty5KleunDZv3qxWrVq59pcoUUIhISFXvcby5cv1448/auXKlQoODlajRo30/PPPa/To0Zo4caK8vb3zVAvDBwAAuEFmZqbS09NNW2Zm5nUfl5aWJkkKCgoy7V+wYIHKlCmjevXqacyYMTp37pzrWGJiourXr6/g4GDXvg4dOig9PV27du3Kc800BQAA27pynP5WbgkJCfL39zdtCQkJf1hPTk6OnnrqKbVo0UL16tVz7e/bt6/eeecdff311xozZozmz5+vhx56yHU8JSXF1BBIcv2ckpKS5/eD4QMAANxgzJgxio+PN+1zOp1/+JjY2Fjt3LlT69atM+1/5JFHXH+uX7++QkND1a5dO+3fv1/VqlW7ZTXTFAAAbMudcwqcTud1m4Dfi4uL09KlS7V27VpVqFDhD89t3ry5JCkpKUnVqlVTSEiIvv/+e9M5qampknTNeQhXw/ABAAAWMgxDcXFx+vjjj7V69WpVqVLluo/Ztm2bJCk0NFSSFBkZqR07dujo0aOuc1asWCE/Pz9FRETkuRaSAgCAfVm/+ECxsbFauHChPvnkE/n6+rrmAPj7+8vHx0f79+/XwoUL1blzZ5UuXVrbt2/X008/rVatWqlBgwaSpPbt2ysiIkL9+/fX1KlTlZKSorFjxyo2NjZfaQVJAQAAFpo1a5bS0tLUpk0bhYaGurb3339fkuTt7a2VK1eqffv2ql27toYPH67evXvrs88+c13D09NTS5culaenpyIjI/XQQw9pwIABpvsa5AVJAQDAtgrDfQoMw/jD4xUrVtSaNWuue53w8HB98cUXN1ULSQEAAJBEUgAAsLHCkBQUJjQFAADboikwY/gAAABIIikAANgYSYEZSQEAAJBEUgAAsDOCAhOSAgAAIImkAABgY8wpMCMpAAAAkkgKAAA2RlJgRlMAALAtmgIzhg8AAIAkkgIAgJ0RFJiQFAAAAEkkBQAAG2NOgRlJAQAAkERSAACwMZICM0ubgqysLC1ZskSJiYlKSUmRJIWEhOjuu+9W9+7d5e3tbWV5AADYimXDB0lJSapTp45iYmK0detW5eTkKCcnR1u3btWAAQNUt25dJSUlWVUeAMAGHA6H27aiyLKk4LHHHlP9+vW1detW+fn5mY6lp6drwIABio2N1VdffWVRhQCA211R/cvbXSxrCr777jt9//33uRoCSfLz89Pzzz+v5s2bW1AZAAD2ZNnwQUBAgA4ePHjN4wcPHlRAQECB1QMAsCGHG7ciyLKkYMiQIRowYIDGjRundu3aKTg4WJKUmpqqVatWacqUKXr88cetKg8AANuxrCmYPHmySpYsqZdfflnDhw93jesYhqGQkBCNHj1ao0aNsqo8AIANMKfAzNIliaNHj9bo0aOVnJxsWpJYpUoVK8sCAMCWCsXNi6pUqUIjAAAocCQFZtzmGAAASCokSQEAAFYgKTCjKQAA2Bc9gQnDBwAAQFIhaAqWLVumdevWuX6eOXOmGjVqpL59++rUqVMWVgYAuN3x3QdmljcFI0eOVHp6uiRpx44dGj58uDp37qzk5GTFx8dbXB0AAPZh+ZyC5ORkRURESJIWL16srl276sUXX9SWLVvUuXNni6sDANzOiupv9O5ieVLg7e2tc+fOSZJWrlyp9u3bS5KCgoJcCQIAAHA/y5OCli1bKj4+Xi1atND333+v999/X5K0d+9eVahQweLqkBfvLVygeXPe0vHjx1SzVm098+w41W/QwOqygOsa0edO9WhRUzUrBul81kVt/PF/eu6ttdr3i3k+U/M6oZo48B41qx2q7OwcbT9wVN2eXayMrIuSpOrlA/Xi0NaKjAiTt5endiYf06R/f6e1P/zXipeFfCApMLM8KXjjjTfk5eWlDz/8ULNmzVL58uUlSV9++aU6duxocXW4nmVffqFpUxM07C+xem/Rx6pVq7YeGzZYJ06csLo04LruaVBRsz/bqtZPLVDXMYvk5emppS/erxLOYq5zmtcJ1Scv3KdVmw/qnifeUcsn3tHsT7cpxzBc53w0uae8PDzUafQHujtuvrYfOKaPJvdScGAJK14WcMMchvG7T/ZtIuOi1RXYR7/o+1W3Xn09O3a8JCknJ0ft27XWg337a/DQRyyu7vYW2GWa1SXcdsr4++i/H8Qqavh7+m7nL5KkNTP6atWWnzX5399d9TGl/Xz0y6JYRQ1/V9/t/J8kqZRPMR1b8qQ6P/OBvt56qMDqvx2d/2qEW69f5anP3Xbt5Bld3HZtd7E8KdiyZYt27Njh+vmTTz5Rjx499OyzzyorK8vCynA9F7KytPvHXbor8m7XPg8PD911193a/sNWCysDboxfSack6dSvGZKksv4ldGedMB07fU5fT39QB997TMtf7qO765Z3PeZE+nnt+e8J9Y2qqxLOYvL0cGhIl4ZKPXVWW/elWvI6kA8ON25FkOVNwbBhw7R3715J0oEDBxQdHa0SJUpo0aJFefrq5MzMTKWnp5u2zMxMd5cNSadOn1J2drZKly5t2l+6dGkdP37coqqAG+NwSC8/2lbrd/6iH3++9PmtEuovSXqu/916+8sd6v7cYm1LStUXL92vamEBrsd2eWaRGlYrp2NLntDppU/riV5N1f25xTp9hv8XoWixvCnYu3evGjVqJElatGiRWrVqpYULF2ru3LlavHjxdR+fkJAgf39/0/byXxPcXDWA282MuCjVDS+jAQlLXfs8PC79uvfWFz9o/vKd+mH/UY36+zfa+8spxXSo7zpvelyUjp0+p6jh7+qeJ97Rp+uTtHhST4UElSzw14H84eZFZpavPjAMQzk5OZIuLUns2rWrJKlixYp5+m1zzJgxuW5yZHg6b32hyCUwIFCenp65JhWeOHFCZcqUsagqIP+mx7ZT5+ZVFTX8ff3v+BnX/iMnzkqSdv9s/ozv+e8JVSznK0lq06iSOt9ZVaH3vaFfz10a8nzqjZVq1zhcD0XV1bQPvi+gVwHcPMuTgqZNm2rKlCmaP3++1qxZoy5dLk3MSE5OVnBw8HUf73Q65efnZ9qcTpqCglDM21t1Iupq44ZE176cnBxt3JioBg3vsLAyIO+mx7bTvXdXV8dRH+jn1DTTsZ9T03T4+K+qWSHItL96+UAdOnrpPiolnJd+t8rJMc/Zzskx5PAomr8t2glJgZnlScGMGTPUr18/LVmyRM8995yqV68uSfrwww919913X+fRsFr/mEEa9+xo1a1bT/XqN9A78+fp/Pnz6tGzl9WlAdc1Iy5KfdrW1v0Tl+jM+SzXEsK0s1muexBM/3CTxvZvoR0HjumHA0f1UFRd1aoYpL5TPpUkbdx9RKfOZOhfIzvpxQWJOp95UQ93aqDKIf5a9v0By14bcCMK7ZLEjIwMeXp6qlixYtc/+crHsiSxQL274B3XzYtq1a6j0c+OVYMGDa0u67bHksSbd63lbkOnfal3Vuxy/TzigTs17N5GCvT10Y4DR/Xcv9Zq/a7/uY43rhGsiQNbqnHNEBXz9NDun0/oxQWJWv6fZLe/htudu5ckVh/xpduunTStk9uu7S6Ftim4GTQFsAOaAtgBTUHBsnz4IDs7W9OnT9cHH3ygQ4cO5bo3wcmTJy2qDABwuyuqY//uYvlEw0mTJumVV15Rnz59lJaWpvj4ePXq1UseHh6aOHGi1eUBAG5jDof7tqLI8qZgwYIF+uc//6nhw4fLy8tLDz74oP71r39p/Pjx2rBhg9XlAQBgG5Y3BSkpKapf/9JNQEqVKqW0tEtLgrp27arPP3ffPakBAGBJopnlTUGFChV05MgRSVK1atW0fPlySdKmTZu43wAAAAXI8qagZ8+eWrVqlSTp8ccf17hx41SjRg0NGDBADz/8sMXVAQBuZ8wpMLN89cFLL73k+nOfPn1UqVIlJSYmqkaNGurWrZuFlQEAYC+WNwVXioyMVGRkpNVlAABswINbUZtY0hR8+umneT733nvvdWMlAADgMkuagh49euTpPIfDoezsbPcWAwCwraI69u8uljQFl78qGQAAKxXVpYPuYvnqAwAAUDhY1hSsXr1aERERSk9Pz3UsLS1NdevW1dq1ay2oDABgFyxJNLOsKZgxY4aGDh0qPz+/XMf8/f01bNgwTZ8+3YLKAACwJ8uagh9++EEdO3a85vH27dtr8+bNBVgRAMBuuM2xmWVNQWpqqooVK3bN415eXjp27FgBVgQAgL1Z1hSUL19eO3fuvObx7du3KzQ0tAArAgDYDUmBmWVNQefOnTVu3DhlZGTkOnb+/HlNmDBBXbt2taAyAADsybLbHI8dO1YfffSRatasqbi4ONWqVUuS9NNPP2nmzJnKzs7Wc889Z1V5AAAbKKK/0LuNZU1BcHCw1q9fr8cee0xjxoyRYRiSLkU5HTp00MyZMxUcHGxVeQAAGyiqMb+7WPqFSOHh4friiy906tQpJSUlyTAM1ahRQ4GBgVaWBQCALRWKOxoGBgaqWbNmuvPOO2kIAAAFpjDcvCghIUHNmjWTr6+vypUrpx49emjPnj2mczIyMhQbG6vSpUurVKlS6t27t1JTU03nHDp0SF26dFGJEiVUrlw5jRw5UhcvXszX+1EomgIAAOxqzZo1io2N1YYNG7RixQpduHBB7du319mzZ13nPP300/rss8+0aNEirVmzRocPH1avXr1cx7Ozs9WlSxdlZWVp/fr1mjdvnubOnavx48fnqxaHcXkw/zaSkb/GCCiSArtMs7oEwO3OfzXCrddv8vzXbrv25nFtb+hxx44dU7ly5bRmzRq1atVKaWlpKlu2rBYuXKj77rtP0qVJ+XXq1FFiYqLuuusuffnll+ratasOHz7smo83e/ZsjR49WseOHZO3t3eenpukAAAAN8jMzFR6erppy8zMvO7j0tLSJElBQUGSpM2bN+vChQuKiopynVO7dm1VqlRJiYmJkqTExETVr1/fNEG/Q4cOSk9P165du/JcM00BAMC23DmnICEhQf7+/qYtISHhD+vJycnRU089pRYtWqhevXqSpJSUFHl7eysgIMB0bnBwsFJSUlznXLli7/LPl8/JC0tXHwAAcLsaM2aM4uPjTfucTucfPiY2NlY7d+7UunXr3FnaNdEUAABsy533KXA6nddtAn4vLi5OS5cu1dq1a1WhQgXX/pCQEGVlZen06dOmtCA1NVUhISGuc77//nvT9S6vTrh8Tl4wfAAAgIUMw1BcXJw+/vhjrV69WlWqVDEdb9KkiYoVK6ZVq1a59u3Zs0eHDh1SZGSkJCkyMlI7duzQ0aNHXeesWLFCfn5+ioiIyHMtJAUAANsqDDc0jI2N1cKFC/XJJ5/I19fXNQfA399fPj4+8vf31+DBgxUfH6+goCD5+fnp8ccfV2RkpO666y5JUvv27RUREaH+/ftr6tSpSklJ0dixYxUbG5uvtIKmAABgW4XhNsezZs2SJLVp08a0f86cORo4cKAkafr06fLw8FDv3r2VmZmpDh066M0333Sd6+npqaVLl+qxxx5TZGSkSpYsqZiYGE2ePDlftXCfAqCI4j4FsAN336egecIat11745jWbru2u5AUAABsqxAEBYUKEw0BAIAkkgIAgI0VhjkFhQlJAQAAkERSAACwMYICM5ICAAAgiaQAAGBjzCkwoykAANgWPYEZwwcAAEASSQEAwMYYPjAjKQAAAJJICgAANkZSYEZSAAAAJJEUAABsjKDAjKQAAABIIikAANgYcwrMaAoAALZFT2DG8AEAAJBEUgAAsDGGD8xICgAAgCSSAgCAjREUmJEUAAAASSQFAAAb8yAqMCEpAAAAkkgKAAA2RlBgRlMAALAtliSaMXwAAAAkkRQAAGzMg6DAhKQAAABIIikAANgYcwrMSAoAAIAkkgIAgI0RFJiRFAAAAEkkBQAAG3OIqOD3aAoAALbFkkQzhg8AAIAkkgIAgI2xJNGMpAAAAEgiKQAA2BhBgRlJAQAAkERSAACwMQ+iAhOSAgAAIImkAABgYwQFZjQFAADbYkmiGcMHAABAEkkBAMDGCArMSAoAAIAkkgIAgI2xJNGMpAAAAEgiKQAA2Bg5gRlJAQAAkERSAACwMe5TYEZTAACwLQ96AhOGDwAAgCSSAgCAjTF8YEZSAAAAJJEUAABsjKDAjKQAAABIIikAANgYcwrMSAoAAIAkkgIAgI1xnwIzmgIAgG0xfGDG8AEAAJBEUgAAsDFyAjOSAgAAIOkGm4Jvv/1WDz30kCIjI/W///1PkjR//nytW7fulhYHAIA7eTgcbtuKonw3BYsXL1aHDh3k4+OjrVu3KjMzU5KUlpamF1988ZYXCAAACka+m4IpU6Zo9uzZ+uc//6lixYq59rdo0UJbtmy5pcUBAOBODof7tqIo303Bnj171KpVq1z7/f39dfr06VtREwAAtrJ27Vp169ZNYWFhcjgcWrJkien4wIED5XA4TFvHjh1N55w8eVL9+vWTn5+fAgICNHjwYJ05cyZfdeS7KQgJCVFSUlKu/evWrVPVqlXzezkAACxz5V+0t3LLj7Nnz6phw4aaOXPmNc/p2LGjjhw54treffdd0/F+/fpp165dWrFihZYuXaq1a9fqkUceyVcd+V6SOHToUD355JN6++235XA4dPjwYSUmJmrEiBEaN25cfi8HAIDtderUSZ06dfrDc5xOp0JCQq56bPfu3Vq2bJk2bdqkpk2bSpJef/11de7cWdOmTVNYWFie6sh3U/DMM88oJydH7dq107lz59SqVSs5nU6NGDFCjz/+eH4vBwCAZdw59p+ZmemajH+Z0+mU0+m8oet98803KleunAIDA/WnP/1JU6ZMUenSpSVJiYmJCggIcDUEkhQVFSUPDw9t3LhRPXv2zNNz5Hv4wOFw6LnnntPJkye1c+dObdiwQceOHdPzzz+f30sBAGApdy5JTEhIkL+/v2lLSEi4oTo7duyof//731q1apX++te/as2aNerUqZOys7MlSSkpKSpXrpzpMV5eXgoKClJKSkqen+eG72jo7e2tiIiIG304AAC3tTFjxig+Pt6070ZTgujoaNef69evrwYNGqhatWr65ptv1K5du5uq8/fy3RS0bdv2DydQrF69+qYKAgCgoLhz+OBmhgqup2rVqipTpoySkpLUrl07hYSE6OjRo6ZzLl68qJMnT15zHsLV5LspaNSokennCxcuaNu2bdq5c6diYmLyezkAAJBPv/zyi06cOKHQ0FBJUmRkpE6fPq3NmzerSZMmki79kp6Tk6PmzZvn+br5bgqmT59+1f0TJ07M93pIAACsVFi+OvnMmTOm5f7Jycnatm2bgoKCFBQUpEmTJql3794KCQnR/v37NWrUKFWvXl0dOnSQJNWpU0cdO3bU0KFDNXv2bF24cEFxcXGKjo7O88oD6RZ+IdJDDz2kt99++1ZdDgAA2/jPf/6jO+64Q3fccYckKT4+XnfccYfGjx8vT09Pbd++Xffee69q1qypwYMHq0mTJvr2229NwxMLFixQ7dq11a5dO3Xu3FktW7bUP/7xj3zVccu+OjkxMVHFixe/VZcDcD1HD1pdAVDkFZavCm7Tpo0Mw7jm8a+++uq61wgKCtLChQtvqo58NwW9evUy/WwYho4cOaL//Oc/3LwIAIAiLN9Ngb+/v+lnDw8P1apVS5MnT1b79u1vWWEAALhbYZlTUFjkqynIzs7WoEGDVL9+fQUGBrqrJgAACoQHPYFJvoZTPD091b59e74NEQCA21C+51jUq1dPBw4ccEctAAAUKA+H+7aiKN9NwZQpUzRixAgtXbpUR44cUXp6umkDAABFU57nFEyePFnDhw9X586dJUn33nuvaYKGYRhyOByuL2cAAKCwY6KhWZ6bgkmTJunRRx/V119/7c56AACARfLcFFy+qULr1q3dVgwAAAWpqI79u0u+5hQQswAAcPvK130Katased3G4OTJkzdVEAAABYXfdc3y1RRMmjQp1x0NAQAoqjzoCkzy1RRER0erXLly7qoFAABYKM9NAfMJAAC3m8LyLYmFRZ7fjz/6SkcAAFD05TkpyMnJcWcdAAAUOEJwM5ITAAAgKZ8TDQEAuJ2w+sCMpAAAAEgiKQAA2BhBgRlNAQDAtvjuAzOGDwAAgCSSAgCAjTHR0IykAAAASCIpAADYGEGBGUkBAACQRFIAALAxVh+YkRQAAABJJAUAABtziKjg92gKAAC2xfCBGcMHAABAEkkBAMDGSArMSAoAAIAkkgIAgI05uHuRCUkBAACQRFIAALAx5hSYkRQAAABJJAUAABtjSoEZTQEAwLY86ApMGD4AAACSSAoAADbGREMzkgIAACCJpAAAYGNMKTAjKQAAAJJICgAANuYhooLfIykAAACSSAoAADbGnAIzmgIAgG2xJNGM4QMAACCJpAAAYGPc5tiMpAAAAEgiKQAA2BhBgRlJAQAAkERSAACwMeYUmJEUAAAASSQFAAAbIygwoykAANgWcbkZ7wcAAJBEUgAAsDEH4wcmJAUAAEASSQEAwMbICcxICgAAgCSSAgCAjXHzIjOSAgAAIImkAABgY+QEZjQFAADbYvTAjOEDAAAgiaQAAGBj3LzIjKQAAACLrV27Vt26dVNYWJgcDoeWLFliOm4YhsaPH6/Q0FD5+PgoKipK+/btM51z8uRJ9evXT35+fgoICNDgwYN15syZfNVBUwAAsC0PN275cfbsWTVs2FAzZ8686vGpU6fqtdde0+zZs7Vx40aVLFlSHTp0UEZGhuucfv36adeuXVqxYoWWLl2qtWvX6pFHHslXHQ7DMIx81l7oZVy0ugLA/QKbxVldAuB257e+4dbrv7/1f267dp87yt/Q4xwOhz7++GP16NFD0qWUICwsTMOHD9eIESMkSWlpaQoODtbcuXMVHR2t3bt3KyIiQps2bVLTpk0lScuWLVPnzp31yy+/KCwsLE/PTVIAALAth8Phti0zM1Pp6emmLTMzM981JicnKyUlRVFRUa59/v7+at68uRITEyVJiYmJCggIcDUEkhQVFSUPDw9t3Lgxz89FUwAAgBskJCTI39/ftCUkJOT7OikpKZKk4OBg0/7g4GDXsZSUFJUrV8503MvLS0FBQa5z8oLVBwAA23Ln2oMxY8YoPj7etM/pdLrxGW8eTQEAAG7gdDpvSRMQEhIiSUpNTVVoaKhrf2pqqho1auQ65+jRo6bHXbx4USdPnnQ9Pi8YPgAA2JY75xTcKlWqVFFISIhWrVrl2peenq6NGzcqMjJSkhQZGanTp09r8+bNrnNWr16tnJwcNW/ePM/PRVIAALCtwvKb8ZkzZ5SUlOT6OTk5Wdu2bVNQUJAqVaqkp556SlOmTFGNGjVUpUoVjRs3TmFhYa4VCnXq1FHHjh01dOhQzZ49WxcuXFBcXJyio6PzvPJAoikAAMBy//nPf9S2bVvXz5fnIsTExGju3LkaNWqUzp49q0ceeUSnT59Wy5YttWzZMhUvXtz1mAULFiguLk7t2rWTh4eHevfurddeey1fdXCfAqCI4j4FsAN336fg4+15n5mfXz0b5H0sv7AoLMkJAACwGMMHAADb4uuQzEgKAACAJJICAICN8c3JZiQFAABAEkkBAMDGPJhVYEJTAACwLYYPzBg+AAAAkkgKAAA25mD4wISkAAAASCIpAADYGHMKzEgKAACAJJICAICNsSTRrNAmBampqZo8ebLVZQAAYBuFtilISUnRpEmTrC4DAHAbczjctxVFlg0fbN++/Q+P79mzp4AqAQDYVVH9y9tdLGsKGjVqJIfDIcMwch27vN/Bvy0AAAqMZU1BUFCQpk6dqnbt2l31+K5du9StW7cCrgoAYCfcvMjMsqagSZMmOnz4sMLDw696/PTp01dNEQAAgHtY1hQ8+uijOnv27DWPV6pUSXPmzCnAigAAduNBUGBiWVPQs2fPPzweGBiomJiYAqoGAABw8yIAgG0xp8Cs0N6nAAAAFCySAgCAbbHy3YymAABgWwwfmDF8AAAAJBWCpmDZsmVat26d6+eZM2eqUaNG6tu3r06dOmVhZQCA252Hw31bUWR5UzBy5Eilp6dLknbs2KHhw4erc+fOSk5OVnx8vMXVAQBgH5bPKUhOTlZERIQkafHixeratatefPFFbdmyRZ07d7a4OgDA7Yw5BWaWJwXe3t46d+6cJGnlypVq3769pEvfjXA5QQAAAO5neVLQsmVLxcfHq0WLFvr+++/1/vvvS5L27t2rChUqWFwd8uK9hQs0b85bOn78mGrWqq1nnh2n+g0aWF0WcF0jHm6vHn9qqJqVg3U+84I2/nBAz736ifb9fNR1TpUKZfTS0z0VeUdVOYt5acX63Yr/6yIdPfmr65xFM4apYc3yKhvkq1Pp5/T1xj0a+9onOnIszYqXhXxgSaKZ5UnBG2+8IS8vL3344YeaNWuWypcvL0n68ssv1bFjR4urw/Us+/ILTZuaoGF/idV7iz5WrVq19diwwTpx4oTVpQHXdU/j6pr9/lq1HjBNXR97Q15enlo6K04lintLkkoU99bSN2NlGIY6PfK6/jRouryLeWrxq8NMX+2+dtNePTT6bTXsOVl9R/5LVSuW0cKXB1v1soAb5jBuw68izLhodQX20S/6ftWtV1/Pjh0vScrJyVH7dq31YN/+Gjz0EYuru70FNouzuoTbTpnAUvrv6pcUNXi6vtuyX+3uqq1P3viLQluP0q9nMyRJfqWK68iaqer6l5n6euOeq16nS+v6+uCVofJv/pQuXswpyJdw2zm/9Q23Xv+7fe5b5daiRqDbru0ulicFW7Zs0Y4dO1w/f/LJJ+rRo4eeffZZZWVlWVgZrudCVpZ2/7hLd0Xe7drn4eGhu+66W9t/2GphZcCN8StVXJJ0Ku3SPCent5cMw1Bm1v//ppGReVE5OYbublTtqtcI9Cuh6E5NteGHZBqCIsDD4XDbVhRZ3hQMGzZMe/fulSQdOHBA0dHRKlGihBYtWqRRo0Zd9/GZmZlKT083bZmZme4uG5JOnT6l7OxslS5d2rS/dOnSOn78uEVVATfG4XDo5RH3af3W/fpx/xFJ0vc7Durs+Sy98GR3+RQvphLFvfVSfE95eXkqpIyf6fFTnuiu4+v/psNrpqpiaJDuf/ofVrwM4KZY3hTs3btXjRo1kiQtWrRIrVq10sKFCzV37lwtXrz4uo9PSEiQv7+/aXv5rwlurhrA7WbGmAdUt3qoBjwzx7Xv+Kkz6jfqLXVuVU/Hv/ubUr99Wf6lfLTlx0PKuWLkdfq/V+qu6L+qy6NvKDs7R/96vn9BvwTcAIcbt6LI8tUHhmEoJ+dSxLZy5Up17dpVklSxYsU8/bY5ZsyYXDc5Mjydt75Q5BIYEChPT89ckwpPnDihMmXKWFQVkH/TR9+vzvfUU9TgGfrf0dOmY6s2/KS6905S6YCSungxR2lnzit5xYs6+NVm03knTp/VidNnlXToqPYkpyjpqylq3qCKNm5PLsBXAtwcy5OCpk2basqUKZo/f77WrFmjLl26SLp0U6Pg4ODrPt7pdMrPz8+0OZ00BQWhmLe36kTU1cYNia59OTk52rgxUQ0a3mFhZUDeTR99v+79U0N1HPaafj587VUzJ06fVdqZ82rdrKbKBZXS0jU7rnmux2/3uPUuZvnvXbgeogITyz+xM2bMUL9+/bRkyRI999xzql69uiTpww8/1N13332dR8Nq/WMGadyzo1W3bj3Vq99A78yfp/Pnz6tHz15WlwZc14wxD6hPp6a6/+l/6MzZDAWX9pUkpZ3JUEbmBUlS/3vv0p7kFB07dUbNG1TRtJH36fUFX7vuZdCsXria1A3X+q37dfrXc6pSoawm/KWL9h86RkqAIqfQLknMyMiQp6enihUrlv/HsiSxQL274B3XzYtq1a6j0c+OVYMGDa0u67bHksSbd63lbkPHz9c7n22UJD3/xL16qNtdCvIvoZ8Pn9S/Plyn195Z7Tq3bvUwTRvZW/VrVlBJH2+lHE/T8vW79dd/LtNhbl5009y9JHHjfvf9O2pezd9t13aXQtsU3AyaAtgBTQHsgKagYFk+fJCdna3p06frgw8+0KFDh3Ldm+DkyZMWVQYAuN0V0dsJuI3lEw0nTZqkV155RX369FFaWpri4+PVq1cveXh4aOLEiVaXBwC4jTHP0MzypmDBggX65z//qeHDh8vLy0sPPvig/vWvf2n8+PHasGGD1eUBAGAbljcFKSkpql+/viSpVKlSSku7NL7TtWtXff7551aWBgC43REVmFjeFFSoUEFHjly6pWi1atW0fPlySdKmTZu43wAAAAXI8qagZ8+eWrVqlSTp8ccf17hx41SjRg0NGDBADz/8sMXVAQBuZw43/lMUWb764KWXXnL9uU+fPqpUqZISExNVo0YNdevWzcLKAACwF8ubgitFRkYqMjLS6jIAADbAkkQzS5qCTz/9NM/n3nvvvW6sBAAAXGZJU9CjR488nedwOJSdne3eYgAAtkVQYGZJU3D5q5IBALAUXYGJ5asPAABA4WBZU7B69WpFREQoPT0917G0tDTVrVtXa9eutaAyAIBdsCTRzLKmYMaMGRo6dKj8/PxyHfP399ewYcM0ffp0CyoDAMCeLGsKfvjhB3Xs2PGax9u3b6/NmzcXYEUAALtxONy3FUWWNQWpqakqVqzYNY97eXnp2LFjBVgRAAD2ZllTUL58ee3cufOax7dv367Q0NACrAgAYDd8H5KZZU1B586dNW7cOGVkZOQ6dv78eU2YMEFdu3a1oDIAAOzJYRiGYcUTp6amqnHjxvL09FRcXJxq1aolSfrpp580c+ZMZWdna8uWLQoODs73tTMu3upqgcInsFmc1SUAbnd+6xtuvf4P//3VbdduWNHXbdd2F8u++yA4OFjr16/XY489pjFjxuhyb+JwONShQwfNnDnzhhoCAADyqqguHXQXS78QKTw8XF988YVOnTqlpKQkGYahGjVqKDAw0MqyAACwpULxLYmBgYFq1qyZ1WUAAGymqC4ddBducwwAACQVkqQAAAArEBSYkRQAAABJJAUAADsjKjAhKQAAAJJoCgAANlYYvjp54sSJcjgcpq127dqu4xkZGYqNjVXp0qVVqlQp9e7dW6mpqe54O2gKAACwWt26dXXkyBHXtm7dOtexp59+Wp999pkWLVqkNWvW6PDhw+rVq5db6mBOAQDAtgrLfQq8vLwUEhKSa39aWpreeustLVy4UH/6058kSXPmzFGdOnW0YcMG3XXXXbe0DpICAIBtufNbEjMzM5Wenm7aMjMzr1rHvn37FBYWpqpVq6pfv346dOiQJGnz5s26cOGCoqKiXOfWrl1blSpVUmJi4q19M0RTAACAWyQkJMjf39+0JSQk5DqvefPmmjt3rpYtW6ZZs2YpOTlZ99xzj3799VelpKTI29tbAQEBpscEBwcrJSXlltfM8AEAwL7cOHwwZswYxcfHm/Y5nc5c53Xq1Mn15wYNGqh58+YKDw/XBx98IB8fH/cVeBUkBQAAuIHT6ZSfn59pu1pTcKWAgADVrFlTSUlJCgkJUVZWlk6fPm06JzU19apzEG4WTQEAwLYKw5LEK505c0b79+9XaGiomjRpomLFimnVqlWu43v27NGhQ4cUGRl5K94CE4YPAACw0IgRI9StWzeFh4fr8OHDmjBhgjw9PfXggw/K399fgwcPVnx8vIKCguTn56fHH39ckZGRt3zlgURTAACwscKwJPGXX37Rgw8+qBMnTqhs2bJq2bKlNmzYoLJly0qSpk+fLg8PD/Xu3VuZmZnq0KGD3nzzTbfU4jAMw3DLlS2UcdHqCgD3C2wWZ3UJgNud3/qGW6+/J+Wc265dK6SE267tLiQFAADbKgRBQaFCUwAAsC+6AhNWHwAAAEkkBQAAG7uZpYO3I5ICAAAgiaQAAGBjhWFJYmFCUgAAACSRFAAAbIygwIykAAAASCIpAADYGVGBCU0BAMC2WJJoxvABAACQRFIAALAxliSakRQAAABJJAUAABsjKDAjKQAAAJJICgAAdkZUYEJSAAAAJJEUAABsjPsUmNEUAABsiyWJZgwfAAAASSQFAAAbIygwIykAAACSSAoAADbGnAIzkgIAACCJpAAAYGtEBb9HUgAAACSRFAAAbIw5BWY0BQAA26InMGP4AAAASCIpAADYGMMHZiQFAABAEkkBAMDG+JZEM5ICAAAgiaQAAGBnBAUmJAUAAEASSQEAwMYICsxoCgAAtsWSRDOGDwAAgCSSAgCAjbEk0YykAAAASCIpAADYGUGBCUkBAACQRFIAALAxggIzkgIAACCJpAAAYGPcp8CMpgAAYFssSTRj+AAAAEgiKQAA2BjDB2YkBQAAQBJNAQAA+A1NAQAAkMScAgCAjTGnwIykAAAASCIpAADYGPcpMKMpAADYFsMHZgwfAAAASSQFAAAbIygwIykAAACSSAoAAHZGVGBCUgAAACSRFAAAbIwliWYkBQAAQBJJAQDAxrhPgRlJAQAAkERSAACwMYICM5oCAIB90RWYMHwAAAAk0RQAAGzM4cZ/8mvmzJmqXLmyihcvrubNm+v77793wyv+YzQFAABY7P3331d8fLwmTJigLVu2qGHDhurQoYOOHj1aoHXQFAAAbMvhcN+WH6+88oqGDh2qQYMGKSIiQrNnz1aJEiX09ttvu+eFXwNNAQAAbpCZman09HTTlpmZmeu8rKwsbd68WVFRUa59Hh4eioqKUmJiYkGWfHuuPih+W76qwiszM1MJCQkaM2aMnE6n1eXYxvmtb1hdgq3wOb89ufPvi4lTEjRp0iTTvgkTJmjixImmfcePH1d2draCg4NN+4ODg/XTTz+5r8CrcBiGYRToM+K2k56eLn9/f6WlpcnPz8/qcgC34HOO/MrMzMyVDDidzlxN5eHDh1W+fHmtX79ekZGRrv2jRo3SmjVrtHHjxgKpV7pNkwIAAKx2tQbgasqUKSNPT0+lpqaa9qempiokJMRd5V0VcwoAALCQt7e3mjRpolWrVrn25eTkaNWqVabkoCCQFAAAYLH4+HjFxMSoadOmuvPOOzVjxgydPXtWgwYNKtA6aApw05xOpyZMmMDkK9zW+JzDnfr06aNjx45p/PjxSklJUaNGjbRs2bJckw/djYmGAABAEnMKAADAb2gKAACAJJoCAADwG5oCmDgcDi1ZssTqMgC34nMOXB1NgY2kpKTo8ccfV9WqVeV0OlWxYkV169bNtDbWSoZhaPz48QoNDZWPj4+ioqK0b98+q8tCEVPYP+cfffSR2rdvr9KlS8vhcGjbtm1WlwS40BTYxMGDB9WkSROtXr1aL7/8snbs2KFly5apbdu2io2Ntbo8SdLUqVP12muvafbs2dq4caNKliypDh06KCMjw+rSUEQUhc/52bNn1bJlS/31r3+1uhQgNwO20KlTJ6N8+fLGmTNnch07deqU68+SjI8//tj186hRo4waNWoYPj4+RpUqVYyxY8caWVlZruPbtm0z2rRpY5QqVcrw9fU1GjdubGzatMkwDMM4ePCg0bVrVyMgIMAoUaKEERERYXz++edXrS8nJ8cICQkxXn75Zde+06dPG06n03j33Xdv8tXDLgr75/z3kpOTDUnG1q1bb/j1ArcaNy+ygZMnT2rZsmV64YUXVLJkyVzHAwICrvlYX19fzZ07V2FhYdqxY4eGDh0qX19fjRo1SpLUr18/3XHHHZo1a5Y8PT21bds2FStWTJIUGxurrKwsrV27ViVLltSPP/6oUqVKXfV5kpOTlZKSYvrqUH9/fzVv3lyJiYmKjo6+iXcAdlAUPudAYUdTYANJSUkyDEO1a9fO92PHjh3r+nPlypU1YsQIvffee67/WR46dEgjR450XbtGjRqu8w8dOqTevXurfv36kqSqVate83lSUlIk6apfHXr5GPBHisLnHCjsmFNgA8ZN3LTy/fffV4sWLRQSEqJSpUpp7NixOnTokOt4fHy8hgwZoqioKL300kvav3+/69gTTzyhKVOmqEWLFpowYYK2b99+U68D+CN8zoGbR1NgAzVq1JDD4dBPP/2Ur8clJiaqX79+6ty5s5YuXaqtW7fqueeeU1ZWluuciRMnateuXerSpYtWr16tiIgIffzxx5KkIUOG6MCBA+rfv7927Nihpk2b6vXXX7/qc13+etDC8NWhKJqKwuccKPSsndKAgtKxY8d8T8CaNm2aUbVqVdO5gwcPNvz9/a/5PNHR0Ua3bt2ueuyZZ54x6tevf9VjlycaTps2zbUvLS2NiYbIl8L+Of89JhqiMCIpsImZM2cqOztbd955pxYvXqx9+/Zp9+7deu211675fd01atTQoUOH9N5772n//v167bXXXL8dSdL58+cVFxenb775Rj///LO+++47bdq0SXXq1JEkPfXUU/rqq6+UnJysLVu26Ouvv3Ydu5LD4dBTTz2lKVOm6NNPP9WOHTs0YMAAhYWFqUePHrf8/cDtqbB/zqVLEyK3bdumH3/8UZK0Z88ebdu2jbkzKBys7kpQcA4fPmzExsYa4eHhhre3t1G+fHnj3nvvNb7++mvXObpiqdbIkSON0qVLG6VKlTL69OljTJ8+3fUbVGZmphEdHW1UrFjR8Pb2NsLCwoy4uDjj/PnzhmEYRlxcnFGtWjXD6XQaZcuWNfr3728cP378mvXl5OQY48aNM4KDgw2n02m0a9fO2LNnjzveCtzGCvvnfM6cOYakXNuECRPc8G4A+cNXJwMAAElMNAQAAL+hKQAAAJJoCgAAwG9oCgAAgCSaAgAA8BuaAgAAIImmAAAA/IamAAAASKIpAIqEgQMHmm733KZNGz311FMFXsc333wjh8Oh06dPF/hzA3A/mgLgJgwcOFAOh0MOh0Pe3t6qXr26Jk+erIsXL7r1eT/66CM9//zzeTqXv8gB5JWX1QUARV3Hjh01Z84cZWZm6osvvlBsbKyKFSumMWPGmM7LysqSt7f3LXnOoKCgW3IdAPg9kgLgJjmdToWEhCg8PFyPPfaYoqKi9Omnn7oi/xdeeEFhYWGqVauWJOm///2vHnjgAQUEBCgoKEjdu3fXwYMHXdfLzs5WfHy8AgICVLp0aY0aNUpXfkXJlcMHmZmZGj16tCpWrCin06nq1avrrbfe0sGDB9W2bVtJUmBgoBwOhwYOHChJysnJUUJCgqpUqSIfHx81bNhQH374oel5vvjiC9WsWVM+Pj5q27atqU4Atx+aAuAW8/HxUVZWliRp1apV2rNnj1asWKGlS5fqwoUL6tChg3x9ffXtt9/qu+++U6lSpdSxY0fXY/72t79p7ty5evvtt7Vu3TqdPHnS9FW+VzNgwAC9++67eu2117R79279/e9/V6lSpVSxYkUtXrxY0qWv6D1y5IheffVVSVJCQoL+/e9/a/bs2dq1a5eefvppPfTQQ1qzZo2kS81Lr1691K1bN23btk1DhgzRM8884663DUBhYPG3NAJFWkxMjNG9e3fDMC599fOKFSsMp9NpjBgxwoiJiTGCg4ONzMxM1/nz5883atWqZeTk5Lj2ZWZmGj4+PsZXX31lGIZhhIaGGlOnTnUdv3DhglGhQgXX8xiGYbRu3dp48sknDcMwjD179hiSjBUrVly1xq+//tqQZJw6dcq1LyMjwyhRooSxfv1607mDBw82HnzwQcMwDGPMmDFGRESE6fjo0aNzXQvA7YM5BcBNWrp0qUqVKqULFy4oJydHffv21cSJExUbG6v69eub5hH88MMPSkpKkq+vr+kaGRkZ2r9/v9LS0nTkyBE1b97cdczLy0tNmzbNNYRw2bZt2+Tp6anWrVvnueakpCSdO3dOf/7zn037s7KydMcdd0iSdu/ebapDkiIjI/P8HACKHpoC4Ca1bdtWs2bNkre3t8LCwuTl9f//WZUsWdJ07pkzZ9SkSRMtWLAg13XKli17Q8/v4+OT78ecOXNGkvT555+rfPnypmNOp/OG6gBQ9NEUADepZMmSql69ep7Obdy4sd5//32VK1dOfn5+Vz0nNDRUGzduVKtWrSRJFy9e1ObNm9W4ceOrnl+/fn3l5ORozZo1ioqKynX8clKRnZ3t2hcRESGn06lDhw5dM2GoU6eOPv30U9O+DRs2XP9FAiiymGgIFKB+/fqpTJky6t69u7799lslJyfrm2++0RNPPKFffvlFkvTkk0/qpZde0pIlS/TTTz/pL3/5yx/eY6By5cqKiYnRww8/rCVLlriu+cEHH0iSwsPD5XA4tHTpUh07dkxnzpyRr6+vRowYoaefflrz5s3T/v37tWXLFr3++uuaN2+eJOnRRx/Vvn37NHLkSO3Zs0cLFy7U3Llz3f0WAbAQTQFQgEqUKKG1a9eqUqVK6tWrl+rUqaPBgwcrIyPDlRwMHz5c/fv3V0xMjCIjI+Xr66uePXv+4XVnzZql++67T3/5y19Uu3ZtDR06VGfPnpUklS9fXpMmTdIzzzyj4OBgxcXFSZKef/55jRs3TgkJCapTp446duyozz//XFWqVJEkVapUSYsXL9aSJUvUsGFDzZ49Wy+++KIb3x0AVnMY15q9BAAAbIWkAAAASKIpAAAAv6EpAAAAkmgKAADAb2gKAACAJJoCAADwG5oCAAAgiaYAAAD8hqYAAABIoikAAAC/oSkAAACSpP8Dx6/uAmZblK8AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 600x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Testing loop\n",
    "\n",
    "# Load trained autoencoder\n",
    "trained_autoencoder = CNN_Autoencoder()\n",
    "trained_autoencoder.load_state_dict(torch.load(save_path, weights_only=True))\n",
    "trained_autoencoder.to(device)\n",
    "trained_autoencoder.eval() # Set the model to evaluation mode\n",
    "\n",
    "# Load the model\n",
    "num_classes = 2\n",
    "\n",
    "classifier_model = CNNOnEncoder(trained_autoencoder, num_classes=2).to(device)  # Initialize your classifier model\n",
    "model_state_dict = torch.load('cnn_with_ae_epoch7.pt', weights_only=False)\n",
    "classifier_model.load_state_dict(model_state_dict)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "classifier_model.eval()\n",
    "\n",
    "# Define criterion (loss function)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Test the model\n",
    "correct = 0\n",
    "total = 0\n",
    "total_loss = 0.0\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in testloader:\n",
    "        inputs_reduced = torch.mean(inputs, dim=-1)\n",
    "        inputs = inputs_reduced\n",
    "\n",
    "        # Send inputs and labels to the device (GPU if available)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output = classifier_model(inputs)\n",
    "        # print(outputs.shape)\n",
    "\n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Store predictions and labels for confusion matrix\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Calculate accuracy and average loss\n",
    "accuracy = 100 * correct / total\n",
    "average_loss = total_loss / len(testloader)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# Plot confusion matrix using seaborn\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax,\n",
    "            xticklabels=[f'Class {i}' for i in range(num_classes)],\n",
    "            yticklabels=[f'Class {i}' for i in range(num_classes)])\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('True')\n",
    "ax.set_title('Confusion Matrix')\n",
    "\n",
    "# Print accuracy and average loss\n",
    "print(f'Accuracy of the CNN with autoencoder on the test images: {accuracy:.2f}%')\n",
    "print(f'Average loss on the test images: {average_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EhOKd9tEVyhT"
   },
   "source": [
    "### Not using pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "BrAqWJi3ZhnE"
   },
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm3d(64)  # Batch normalization\n",
    "        self.conv2 = nn.Conv3d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm3d(128)  # Batch normalization\n",
    "        self.conv3 = nn.Conv3d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm3d(256)  # Batch normalization\n",
    "        self.pool = nn.AdaptiveMaxPool3d(output_size=(8, 8, 5))\n",
    "        self.dropout = nn.Dropout(p=0.5)  # Dropout layer\n",
    "        self.fc1 = nn.Linear(256 * 8 * 8 * 5, 256)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))  # First conv + pooling + batch norm\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))  # Second conv + pooling + batch norm\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))  # Third conv + pooling + batch norm\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.dropout(x)  # Apply dropout\n",
    "        x = F.relu(self.fc1(x))  # First fully connected layer\n",
    "        x = self.fc2(x)  # Output layer\n",
    "        return x  # Return logits for CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "tW3TORHwZmS7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([4, 2])\n"
     ]
    }
   ],
   "source": [
    "simple_cnn_model = SimpleCNN(num_classes=2).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(simple_cnn_model.parameters(), lr=0.005, momentum=0.9)\n",
    "\n",
    "example_input = torch.randn(4, 1, 53, 64, 46)\n",
    "example_output = simple_cnn_model(example_input.to(device))\n",
    "print(\"Output shape:\", example_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "Hbl8nB7YV2yZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Train Loss: 0.7379, Train Accuracy: 0.5048\n",
      "Epoch [1/100], Val Loss: 0.6928, Val Accuracy: 0.5223\n",
      "Model saved at epoch 1 with validation loss: 0.6928\n",
      "Epoch [2/100], Train Loss: 0.6938, Train Accuracy: 0.4940\n",
      "Epoch [2/100], Val Loss: 0.6945, Val Accuracy: 0.5223\n",
      "Epoch [3/100], Train Loss: 0.6944, Train Accuracy: 0.5119\n",
      "Epoch [3/100], Val Loss: 0.6934, Val Accuracy: 0.5223\n",
      "Epoch [4/100], Train Loss: 0.6939, Train Accuracy: 0.5179\n",
      "Epoch [4/100], Val Loss: 0.6935, Val Accuracy: 0.5223\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m all_labels \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     11\u001b[0m all_preds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (inputs, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(trainloader):\n\u001b[1;32m     14\u001b[0m     inputs_reduced \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(inputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     15\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m inputs_reduced\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataset.py:418\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitems__\u001b[39m(\u001b[38;5;28mself\u001b[39m, indices: List[\u001b[38;5;28mint\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[T_co]:\n\u001b[1;32m    415\u001b[0m     \u001b[38;5;66;03m# add batched sampling support when parent dataset supports it.\u001b[39;00m\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;66;03m# see torch.utils.data._utils.fetch._MapDatasetFetcher\u001b[39;00m\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)):\n\u001b[0;32m--> 418\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    420\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[0;32mIn[31], line 32\u001b[0m, in \u001b[0;36mFMRI_Dataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     29\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_augmentations(data)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Normalize the data\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Convert to tensor and add missing dimensions\u001b[39;00m\n\u001b[1;32m     35\u001b[0m data_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(data, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[0;32mIn[31], line 83\u001b[0m, in \u001b[0;36mFMRI_Dataset.normalize_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Normalize the data to zero mean and unit variance.\"\"\"\u001b[39;00m\n\u001b[1;32m     82\u001b[0m mean \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m---> 83\u001b[0m std \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m std \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# Avoid division by zero\u001b[39;00m\n\u001b[1;32m     85\u001b[0m     data \u001b[38;5;241m=\u001b[39m (data \u001b[38;5;241m-\u001b[39m mean) \u001b[38;5;241m/\u001b[39m std\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/core/_methods.py:269\u001b[0m, in \u001b[0;36m_std\u001b[0;34m(a, axis, dtype, out, ddof, keepdims, where)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_std\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, ddof\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m    268\u001b[0m          where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 269\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43m_var\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mddof\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mddof\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m               \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, mu\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m    273\u001b[0m         ret \u001b[38;5;241m=\u001b[39m um\u001b[38;5;241m.\u001b[39msqrt(ret, out\u001b[38;5;241m=\u001b[39mret)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/core/_methods.py:236\u001b[0m, in \u001b[0;36m_var\u001b[0;34m(a, axis, dtype, out, ddof, keepdims, where)\u001b[0m\n\u001b[1;32m    231\u001b[0m     arrmean \u001b[38;5;241m=\u001b[39m arrmean \u001b[38;5;241m/\u001b[39m rcount\n\u001b[1;32m    233\u001b[0m \u001b[38;5;66;03m# Compute sum of squared deviations from mean\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;66;03m# Note that x may not be inexact and that we need it to be an array,\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;66;03m# not a scalar.\u001b[39;00m\n\u001b[0;32m--> 236\u001b[0m x \u001b[38;5;241m=\u001b[39m asanyarray(\u001b[43marr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43marrmean\u001b[49m)\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(arr\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, (nt\u001b[38;5;241m.\u001b[39mfloating, nt\u001b[38;5;241m.\u001b[39minteger)):\n\u001b[1;32m    239\u001b[0m     x \u001b[38;5;241m=\u001b[39m um\u001b[38;5;241m.\u001b[39mmultiply(x, x, out\u001b[38;5;241m=\u001b[39mx)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "\n",
    "num_epochs = 100\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    simple_cnn_model.train()\n",
    "    total_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    for batch_idx, (inputs, labels) in enumerate(trainloader):\n",
    "        inputs_reduced = torch.mean(inputs, dim=-1)\n",
    "        inputs = inputs_reduced\n",
    "\n",
    "        # Send inputs and labels to the device (GPU if available)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output = simple_cnn_model(inputs)\n",
    "\n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate the loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Store predictions and labels for accuracy calculation\n",
    "        _, preds = torch.max(output, 1)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    # Calculate training accuracy and average loss for the epoch\n",
    "    train_accuracy = accuracy_score(all_labels, all_preds)\n",
    "    avg_train_loss = total_loss / len(trainloader)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}')\n",
    "\n",
    "    # Validation phase\n",
    "    simple_cnn_model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_labels = []\n",
    "    val_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_labels_batch in valloader:\n",
    "            val_inputs_reduced = torch.mean(val_inputs, dim=-1)\n",
    "            val_inputs = val_inputs_reduced.to(device)\n",
    "            val_labels_batch = val_labels_batch.to(device)\n",
    "\n",
    "            # Forward pass for validation data\n",
    "            val_outputs = simple_cnn_model(val_inputs)\n",
    "            loss = criterion(val_outputs, val_labels_batch)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Store predictions and labels for accuracy calculation\n",
    "            _, val_preds_batch = torch.max(val_outputs, 1)\n",
    "            val_labels.extend(val_labels_batch.cpu().numpy())\n",
    "            val_preds.extend(val_preds_batch.cpu().numpy())\n",
    "\n",
    "    # Calculate validation accuracy and average loss\n",
    "    avg_val_loss = val_loss / len(valloader)\n",
    "    val_accuracy = accuracy_score(val_labels, val_preds)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "    # Save the model if validation loss improves\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(simple_cnn_model.state_dict(), 'best_simple_cnn.pt')\n",
    "        print(f'Model saved at epoch {epoch+1} with validation loss: {avg_val_loss:.4f}')\n",
    "\n",
    "    # Save the model after each epoch\n",
    "    torch.save(simple_cnn_model.state_dict(), f'simple_cnn_epoch{epoch}.pt')\n",
    "\n",
    "# Save the final trained model\n",
    "torch.save(simple_cnn_model.state_dict(), 'simple_cnn_final.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "6tjD7zs4ZtyY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the simple CNN on the test images: 52.23%\n",
      "Average loss on the test images: 0.6925\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Testing loop\n",
    "\n",
    "# Load the model\n",
    "classifier_model = SimpleCNN(num_classes=2).to(device)  # Initialize your classifier model\n",
    "model_state_dict = torch.load('simple_cnn_epoch1.pt', weights_only=False)\n",
    "classifier_model.load_state_dict(model_state_dict)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "classifier_model.eval()\n",
    "\n",
    "# Define criterion (loss function)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Test the model\n",
    "correct = 0\n",
    "total = 0\n",
    "total_loss = 0.0\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in testloader:\n",
    "        inputs_reduced = torch.mean(inputs, dim=-1)\n",
    "        inputs = inputs_reduced\n",
    "\n",
    "        # Send inputs and labels to the device (GPU if available)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output = simple_cnn_model(inputs)\n",
    "        # print(outputs.shape)\n",
    "\n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Store predictions and labels for confusion matrix\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Calculate accuracy and average loss\n",
    "accuracy = 100 * correct / total\n",
    "average_loss = total_loss / len(testloader)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# Plot confusion matrix using seaborn\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax,\n",
    "            xticklabels=[f'Class {i}' for i in range(num_classes)],\n",
    "            yticklabels=[f'Class {i}' for i in range(num_classes)])\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('True')\n",
    "ax.set_title('Confusion Matrix')\n",
    "\n",
    "# Print accuracy and average loss\n",
    "print(f'Accuracy of the simple CNN on the test images: {accuracy:.2f}%')\n",
    "print(f'Average loss on the test images: {average_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J2w7Mw4rSe2U"
   },
   "source": [
    "# CNN-LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hv-DovH-a-AR"
   },
   "source": [
    "### Using pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "WKoBTxICbFxY"
   },
   "outputs": [],
   "source": [
    "class CNNLSTMOnAutoencoder(nn.Module):\n",
    "    def __init__(self, autoencoder, hidden_size, num_classes):\n",
    "        super(CNNLSTMOnAutoencoder, self).__init__()\n",
    "        self.encoder = autoencoder.encoder\n",
    "        self.conv1 = nn.Conv3d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv3d(in_channels=512, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv3d(in_channels=256, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.AdaptiveMaxPool3d(output_size=(8, 8, 5))\n",
    "        self.lstm = None\n",
    "        self.fc = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        x = self.encoder(x)\n",
    "        # Apply convolutions, dropout, and pooling layers\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        # print(f'First conv + pooling {x.shape}')\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        # print(f'Second conv + pooling {x.shape}')\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        # print(f'Third conv + pooling {x.shape}')\n",
    "\n",
    "        # Get the final shape after convolutions\n",
    "        conv_output_shape = x.shape\n",
    "        # print(f\"Convolution output shape: {conv_output_shape}\")\n",
    "        \n",
    "        # Extract available dimensions for reshaping\n",
    "        channels, new_height, new_width, new_depth = conv_output_shape\n",
    "\n",
    "        # Calculate the LSTM input size\n",
    "        lstm_input_size = channels * new_height * new_width\n",
    "\n",
    "        # Reshape for LSTM input\n",
    "        x = x.view(batch_size, new_depth, lstm_input_size)\n",
    "\n",
    "        # Initialize LSTM and FC layers if not yet initialized\n",
    "        if self.lstm is None:\n",
    "            self.lstm = nn.LSTM(input_size=lstm_input_size, hidden_size=128, batch_first=True).to(x.device)\n",
    "            self.fc = nn.Linear(in_features=128, out_features=2).to(x.device)\n",
    "\n",
    "        # Pass through LSTM and FC\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        out = F.relu(self.fc(lstm_out[:, -1, :]))\n",
    "        out = F.softmax(out, dim=1)  # Apply softmax for probabilistic output\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained autoencoder\n",
    "trained_autoencoder = CNN_Autoencoder().to(device)\n",
    "trained_autoencoder.load_state_dict(torch.load(save_path, weights_only=True))\n",
    "trained_autoencoder.eval() # Set the model to evaluation mode\n",
    "\n",
    "# Model with pretrained weights\n",
    "cnn_lstm_with_ae = CNNLSTMOnAutoencoder(trained_autoencoder, 128, 2).to(device)\n",
    "\n",
    "# Optionally, freeze the encoder layers, Frozen= false, Unfrozen= true\n",
    "for param in cnn_lstm_with_ae.encoder.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(cnn_lstm_with_ae.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KfevQjv7erK1"
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "\n",
    "num_epochs = 100\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    cnn_lstm_with_ae.train()\n",
    "    total_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    for batch_idx, (inputs, labels) in enumerate(trainloader):\n",
    "        inputs_reduced = torch.mean(inputs, dim=-1)\n",
    "        inputs = inputs_reduced\n",
    "\n",
    "        # Send inputs and labels to the device (GPU if available)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output = cnn_lstm_with_ae(inputs)\n",
    "\n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate the loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Store predictions and labels for accuracy calculation\n",
    "        _, preds = torch.max(output, 1)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    # Calculate training accuracy and average loss for the epoch\n",
    "    train_accuracy = accuracy_score(all_labels, all_preds)\n",
    "    avg_train_loss = total_loss / len(trainloader)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}')\n",
    "\n",
    "    # Validation phase\n",
    "    cnn_lstm_with_ae.eval()\n",
    "    val_loss = 0.0\n",
    "    val_labels = []\n",
    "    val_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_labels_batch in valloader:\n",
    "            val_inputs_reduced = torch.mean(val_inputs, dim=-1)\n",
    "            val_inputs = val_inputs_reduced.to(device)\n",
    "            val_labels_batch = val_labels_batch.to(device)\n",
    "\n",
    "            # Forward pass for validation data\n",
    "            val_outputs = cnn_lstm_with_ae(val_inputs)\n",
    "            loss = criterion(val_outputs, val_labels_batch)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Store predictions and labels for accuracy calculation\n",
    "            _, val_preds_batch = torch.max(val_outputs, 1)\n",
    "            val_labels.extend(val_labels_batch.cpu().numpy())\n",
    "            val_preds.extend(val_preds_batch.cpu().numpy())\n",
    "\n",
    "    # Calculate validation accuracy and average loss\n",
    "    avg_val_loss = val_loss / len(valloader)\n",
    "    val_accuracy = accuracy_score(val_labels, val_preds)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "    # Save the model if validation loss improves\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(cnn_lstm_with_ae.state_dict(), 'best_cnnlstm_with_ae.pt')\n",
    "        print(f'Model saved at epoch {epoch+1} with validation loss: {avg_val_loss:.4f}')\n",
    "\n",
    "    # Save the model after each epoch\n",
    "    torch.save(cnn_lstm_with_ae.state_dict(), f'cnnlstm_with_ae_epoch{epoch}.pt')\n",
    "\n",
    "# Save the final trained model\n",
    "torch.save(cnn_lstm_with_ae.state_dict(), 'cnnlstm_with_ae_final.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oZppsrq1eun2"
   },
   "outputs": [],
   "source": [
    "# Testing loop\n",
    "\n",
    "# Load trained autoencoder\n",
    "trained_autoencoder = CNN_Autoencoder().to(device)\n",
    "trained_autoencoder.load_state_dict(torch.load(save_path, weights_only=True))\n",
    "trained_autoencoder.eval() # Set the model to evaluation mode\n",
    "\n",
    "# Load the model\n",
    "classifier_model = CNNLSTMOnAutoencoder(trained_autoencoder, hidden_size=128, num_classes=2).to(device)  # Initialize your classifier model\n",
    "model_state_dict = torch.load('cnnlstm_with_ae_final.pt', weights_only=False)\n",
    "classifier_model.load_state_dict(model_state_dict)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "classifier_model.eval()\n",
    "\n",
    "# Define criterion (loss function)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Test the model\n",
    "correct = 0\n",
    "total = 0\n",
    "total_loss = 0.0\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in testloader:\n",
    "        inputs_reduced = torch.mean(inputs, dim=-1)\n",
    "        inputs = inputs_reduced\n",
    "\n",
    "        # Send inputs and labels to the device (GPU if available)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output = simple_cnn_model(inputs)\n",
    "        # print(outputs.shape)\n",
    "\n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Store predictions and labels for confusion matrix\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Calculate accuracy and average loss\n",
    "accuracy = 100 * correct / total\n",
    "average_loss = total_loss / len(testloader)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# Plot confusion matrix using seaborn\n",
    "fig, ax = plt.subplots(figsize=(18, 16))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax,\n",
    "            xticklabels=[f'Class {i}' for i in range(num_classes)],\n",
    "            yticklabels=[f'Class {i}' for i in range(num_classes)])\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('True')\n",
    "ax.set_title('Confusion Matrix')\n",
    "\n",
    "# Print accuracy and average loss\n",
    "print(f'Accuracy of the CNN-LSTM with autoencoder on the test images: {accuracy:.2f}%')\n",
    "print(f'Average loss on the test images: {average_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XtCf0ayFa-f5"
   },
   "source": [
    "### Not using pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "otTTcIpabGSF"
   },
   "outputs": [],
   "source": [
    "class SimpleCNNLSTM(nn.Module):\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super(SimpleCNNLSTM, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv3d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv3d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.AdaptiveMaxPool3d(output_size=(8, 8, 5))\n",
    "        self.lstm = None # To be defined later after determining input size\n",
    "        self.fc = None  # To be defined later after determining LSTM output size\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Apply convolutions, dropout, and pooling layers\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        # print(f'First conv + pooling {x.shape}')\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        # print(f'Second conv + pooling {x.shape}')\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        # print(f'Third conv + pooling {x.shape}')\n",
    "\n",
    "        # Get the final shape after convolutions\n",
    "        conv_output_shape = x.shape\n",
    "        # print(f\"Convolution output shape: {conv_output_shape}\")\n",
    "        \n",
    "        # Extract available dimensions for reshaping\n",
    "        channels, new_height, new_width, new_depth = conv_output_shape\n",
    "\n",
    "        # Calculate the LSTM input size\n",
    "        lstm_input_size = channels * new_height * new_width\n",
    "\n",
    "        # Reshape for LSTM input\n",
    "        x = x.view(batch_size, new_depth, lstm_input_size)\n",
    "\n",
    "        # Initialize LSTM and FC layers if not yet initialized\n",
    "        if self.lstm is None:\n",
    "            self.lstm = nn.LSTM(input_size=lstm_input_size, hidden_size=128, batch_first=True).to(x.device)\n",
    "            self.fc = nn.Linear(in_features=128, out_features=2).to(x.device)\n",
    "\n",
    "        # Pass through LSTM and FC\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        out = F.relu(self.fc(lstm_out[:, -1, :]))\n",
    "        out = F.softmax(out, dim=1)  # Apply softmax for probabilistic output\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model without pretrained weights\n",
    "simple_cnnlstm = SimpleCNNLSTM(hidden_size=128, num_classes=2).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(simple_cnnlstm.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gcu0LbVJc65y"
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "\n",
    "num_epochs = 100\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    simple_cnnlstm.train()\n",
    "    total_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    for batch_idx, (inputs, labels) in enumerate(trainloader):\n",
    "        inputs_reduced = torch.mean(inputs, dim=-1)\n",
    "        inputs = inputs_reduced\n",
    "\n",
    "        # Send inputs and labels to the device (GPU if available)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output = simple_cnnlstm(inputs)\n",
    "\n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate the loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Store predictions and labels for accuracy calculation\n",
    "        _, preds = torch.max(output, 1)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    # Calculate training accuracy and average loss for the epoch\n",
    "    train_accuracy = accuracy_score(all_labels, all_preds)\n",
    "    avg_train_loss = total_loss / len(trainloader)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}')\n",
    "\n",
    "    # Validation phase\n",
    "    simple_cnnlstm.eval()\n",
    "    val_loss = 0.0\n",
    "    val_labels = []\n",
    "    val_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_labels_batch in valloader:\n",
    "            val_inputs_reduced = torch.mean(val_inputs, dim=-1)\n",
    "            val_inputs = val_inputs_reduced.to(device)\n",
    "            val_labels_batch = val_labels_batch.to(device)\n",
    "\n",
    "            # Forward pass for validation data\n",
    "            val_outputs = simple_cnnlstm(val_inputs)\n",
    "            loss = criterion(val_outputs, val_labels_batch)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Store predictions and labels for accuracy calculation\n",
    "            _, val_preds_batch = torch.max(val_outputs, 1)\n",
    "            val_labels.extend(val_labels_batch.cpu().numpy())\n",
    "            val_preds.extend(val_preds_batch.cpu().numpy())\n",
    "\n",
    "    # Calculate validation accuracy and average loss\n",
    "    avg_val_loss = val_loss / len(valloader)\n",
    "    val_accuracy = accuracy_score(val_labels, val_preds)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "    # Save the model if validation loss improves\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(simple_cnnlstm.state_dict(), 'best_simple_cnnlstm.pt')\n",
    "        print(f'Model saved at epoch {epoch+1} with validation loss: {avg_val_loss:.4f}')\n",
    "\n",
    "    # Save the model after each epoch\n",
    "    torch.save(simple_cnnlstm.state_dict(), f'simple_cnnlstm_epoch{epoch}.pt')\n",
    "\n",
    "# Save the final trained model\n",
    "torch.save(simple_cnnlstm.state_dict(), 'simple_cnnlstm_final.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8O9R_idQdIef"
   },
   "outputs": [],
   "source": [
    "# Testing loop\n",
    "\n",
    "# Load the model\n",
    "classifier_model = SimpleCNNLSTM(hidden_size=128, num_classes=2).to(device)  # Initialize your classifier model\n",
    "model_state_dict = torch.load('simple_cnnlstm_final.pt', weights_only=False)\n",
    "classifier_model.load_state_dict(model_state_dict)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "classifier_model.eval()\n",
    "\n",
    "# Define criterion (loss function)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Test the model\n",
    "correct = 0\n",
    "total = 0\n",
    "total_loss = 0.0\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in testloader:\n",
    "        inputs_reduced = torch.mean(inputs, dim=-1)\n",
    "        inputs = inputs_reduced\n",
    "\n",
    "        # Send inputs and labels to the device (GPU if available)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output = simple_cnn_model(inputs)\n",
    "        # print(outputs.shape)\n",
    "\n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Store predictions and labels for confusion matrix\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Calculate accuracy and average loss\n",
    "accuracy = 100 * correct / total\n",
    "average_loss = total_loss / len(testloader)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# Plot confusion matrix using seaborn\n",
    "fig, ax = plt.subplots(figsize=(18, 16))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax,\n",
    "            xticklabels=[f'Class {i}' for i in range(num_classes)],\n",
    "            yticklabels=[f'Class {i}' for i in range(num_classes)])\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('True')\n",
    "ax.set_title('Confusion Matrix')\n",
    "\n",
    "# Print accuracy and average loss\n",
    "print(f'Accuracy of the simple CNN-LSTM on the test images: {accuracy:.2f}%')\n",
    "print(f'Average loss on the test images: {average_loss:.4f}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "S-BF1plyaGTM",
    "EhOKd9tEVyhT",
    "XtCf0ayFa-f5"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
